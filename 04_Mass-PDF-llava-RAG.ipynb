{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be32f03-7b01-4bd0-abb1-728bf4b5284b",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa46804-4309-41ff-85ba-904a05e11b6d",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Ollama\n",
    "* `cd /tmp`\n",
    "* `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "* Test, Optional (2GB download): `ollama run llama3.2`, Type `/bye` when done\n",
    "### Install Langchain\n",
    "* `python3.10 -m pip install langchain langchain_community langchain_chroma langchain_ollama llama-index-legacy pypdf langchain-unstructured \"unstructured[pdf]\" --user`\n",
    "### Install SQLite ( >= 3.35.0 required, This will install 3.46 )\n",
    "* `sudo apt install libreadline-dev python3.10-dev`\n",
    "* `wget https://sqlite.org/2024/sqlite-autoconf-3460100.tar.gz`\n",
    "* `tar -xvf sqlite-autoconf-3460100.tar.gz && cd sqlite-autoconf-3460100`\n",
    "* `./configure`\n",
    "* `make`\n",
    "* `sudo make install`\n",
    "* `python3.10 -m pip uninstall pysqlite3`\n",
    "* `python3.10 -m pip install pysqlite3-binary --user`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8accfe-d20f-49f8-a171-5dea85fe90c3",
   "metadata": {},
   "source": [
    "# Build/Expand Document Database + Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b8be64-1186-4fc2-a5e6-ba4b0ed9249f",
   "metadata": {},
   "source": [
    "## State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a98174-c799-4dc2-ba0c-610379602900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ, path\n",
    "import time, sys, json\n",
    "now = time.time\n",
    "\n",
    "environ[\"_RAG_STATE_PATH\"] = \"data/state.json\"\n",
    "\n",
    "RAGstate = {\n",
    "    'libDocs' : list(),\n",
    "    'pages'   : dict(),\n",
    "}\n",
    "\n",
    "def save_state():\n",
    "    global RAGstate\n",
    "    with open( environ[\"_RAG_STATE_PATH\"], 'w' ) as f:\n",
    "        json.dump( RAGstate, f, indent = 4 )\n",
    "\n",
    "def load_state():\n",
    "    global RAGstate\n",
    "    try:\n",
    "        with open( environ[\"_RAG_STATE_PATH\"], 'r' ) as f:\n",
    "            RAGstate = json.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print( f\"Could not load {environ['_RAG_STATE_PATH']}!\\n{e}\" )\n",
    "\n",
    "load_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb7637-9d24-425a-8050-85c03ed0187b",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7beb3242-a5b2-4203-936f-d1dbdd6d46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "def safe_str( data ):\n",
    "    \"\"\"Filters out invalid UTF-8 characters from a string.\"\"\"\n",
    "    return str( data ).encode( 'utf-8', 'ignore' ).decode( 'utf-8' )\n",
    "\n",
    "def gen_ID():\n",
    "    \"\"\" Generate a unique ID \"\"\"\n",
    "    return safe_str( uuid4() )\n",
    "\n",
    "def pull_ollama_model( modelStr ):\n",
    "    \"\"\" Pull a named model from Ollama and store it wherever \"\"\"\n",
    "    print( f\"About to save '{modelStr}'.\\nThis will spew a lot of text on the first run...\" )\n",
    "    os.system( f\"ollama pull {modelStr}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a32057c-f5bd-4aea-a78f-9bb14b28f510",
   "metadata": {},
   "source": [
    "## Copy PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8219dcd5-258d-4463-bb82-bcafaa104e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "from aa_scrape_PDF import copy_pdfs\n",
    "\n",
    "environ[\"_RAG_PDF_SOURCE\"] = \"/media/james/FILEPILE/$_Robust_Planning/Literature/References/storage\"\n",
    "environ[\"_RAG_PDF_DESTIN\"] = \"data/input/pdf\"\n",
    "environ[\"_RAG_PDF_ERROR\"]  = \"data/input/BAD_PDF\"\n",
    "environ[\"_RAG_PAGE_DESTN\"] = \"data/input/pages\"\n",
    "environ[\"_RAG_VERBOSE\"]    =    \"\"\n",
    "environ[\"_RAG_DOC_ADD\"]    =   \"200\"\n",
    "environ[\"_RAG_DOC_LIMIT\"]  = \"10000\"\n",
    "environ[\"_RAG_DOC_DBASE\"]  = \"lit_pdf\"\n",
    "\n",
    "if not path.exists( environ[\"_RAG_PDF_ERROR\"] ):\n",
    "    makedirs( environ[\"_RAG_PDF_ERROR\"] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335fa2a-8447-41b8-aca7-c2b31156624d",
   "metadata": {},
   "source": [
    "## Determine if more docs will be loaded this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcbdc4e5-3a46-4aab-88af-f79effbeb4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11332 vector records exist!\n",
      "200 files will be copied!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient()\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] )\n",
    "\n",
    "environ[\"_RAG_DOCDB_COUNT\" ] = str( collection.count() )\n",
    "environ[\"_RAG_DOCDB_REMAIN\"] = str( min( int(environ[\"_RAG_DOC_LIMIT\"])-len(RAGstate['libDocs']), int(environ[\"_RAG_DOC_ADD\"]) ) )\n",
    "\n",
    "print( f\"{environ['_RAG_DOCDB_COUNT' ]} vector records exist!\" )\n",
    "print( f\"{environ['_RAG_DOCDB_REMAIN']} files will be copied!\" )\n",
    "\n",
    "# FIXME: COUNT UNIQUE DOCUMENT METADATA!\n",
    "# FIXME: DETERMINE WHAT PDFs WERE COPIED BUT NOT READ!\n",
    "\n",
    "copy_pdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b97eeb-bbeb-4b40-b0a2-61b8f7643707",
   "metadata": {},
   "source": [
    "## Load PDFs by page chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393314ba-7acb-46bf-bfb8-640e72407061",
   "metadata": {},
   "source": [
    "### https://python.langchain.com/docs/how_to/document_loader_pdf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5b7d6-c463-4917-89e4-f1aa6c040058",
   "metadata": {},
   "source": [
    "* `python3.10 -m pip install pypdf langchain-unstructured \"unstructured[pdf]\" --user`\n",
    "* `apt install tesseract-ocr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a2cca2-b617-4243-9e82-82e26de66e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 files, 200 to copy!\n",
      "\n",
      "Read 0 pages in 3.287394841512044e-05 minutes!\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from collections import deque\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "if int(environ[\"_RAG_DOCDB_REMAIN\"]) > 0:\n",
    "    bgn = now()\n",
    "    pdfs_drct = environ[\"_RAG_PDF_DESTIN\"]\n",
    "    eror_drct = environ[\"_RAG_PDF_ERROR\" ]\n",
    "    fNames    = [item for item in os.listdir( pdfs_drct ) if (str( item ).split('.')[-1].lower() == 'pdf')]\n",
    "    print( f\"There are {len(fNames)} files, {environ['_RAG_DOCDB_REMAIN']} to copy!\" )\n",
    "    pages  = deque() # Fast append\n",
    "    lastLn = 0\n",
    "    \n",
    "    for i, fNam in enumerate( fNames ):\n",
    "        file_path = str( path.join( pdfs_drct, fNam ) )\n",
    "        if file_path not in RAGstate['libDocs']:\n",
    "            try:\n",
    "                loader    = PyPDFLoader( file_path )\n",
    "                async for page in loader.alazy_load():\n",
    "                    pages.append( page )\n",
    "                print( f\"{i+1}:{len(pages)}\", end = ', ', flush = True )\n",
    "                if len(pages) > lastLn:\n",
    "                    RAGstate['libDocs'].append( file_path )\n",
    "                lastLn = len(pages)\n",
    "            except Exception as e:\n",
    "                print( f\"ERROR:{e}\", end = ', ', flush = True )\n",
    "                try:\n",
    "                    shutil.move( file_path, path.join( eror_drct, fNam ) )\n",
    "                except Exception as e:\n",
    "                    print( f\"FAILED to move {file_path} --to-> {path.join( eror_drct, fNam )}\" )\n",
    "            except asyncio.CancelledError as e:\n",
    "                print( f\"Load operation cancelled by user\" )\n",
    "                raise e\n",
    "                    \n",
    "            \n",
    "    print()\n",
    "    pages = list( pages )\n",
    "    print( f\"Read {len(pages)} pages in {(now()-bgn)/60.0} minutes!\" )\n",
    "\n",
    "save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e216559a-5e4b-4d21-a8ff-6d796e0c5dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"{pages[0].metadata}\\n\")\n",
    "# print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0be2b0-5b1c-4838-b4db-62b518afeac4",
   "metadata": {},
   "source": [
    "## Load the text embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35da1790-0281-4af3-8b81-88560fc16a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "now = time.time\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "pull_ollama_model( \"all-minilm\" )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = \"all-minilm\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd67496-84af-4d78-8e3d-1284822eed3a",
   "metadata": {},
   "source": [
    "## Populate document vector database (of pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91002b1f-26e3-488c-b3d3-e751d1f8f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "\n",
    "import fitz, pymupdf\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def pdf_page_to_base64( pdf_path: str, page_number: int ):\n",
    "    zoom_x       = 1.5  # horizontal zoom\n",
    "    zoom_y       = 1.5  # vertical zoom\n",
    "    mat          = pymupdf.Matrix( zoom_x, zoom_y )\n",
    "    pdf_document = fitz.open( pdf_path )\n",
    "    page         = pdf_document.load_page(page_number - 1)  # input is one-indexed\n",
    "    pix          = page.get_pixmap( matrix = mat )\n",
    "    img          = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    buffer       = io.BytesIO()\n",
    "    \n",
    "    img.save( buffer, format=\"PNG\" )\n",
    "\n",
    "    return base64.b64encode( buffer.getvalue() ).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571f7d54-8795-4b9b-8a8c-66459c4e9d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected IDs to be a non-empty list, got 0 IDs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocIDs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetaDt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdcmnts\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dcmnts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(now()\u001b[38;5;241m-\u001b[39mbgn)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60.0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes!\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     51\u001b[0m save_state()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/Collection.py:81\u001b[0m, in \u001b[0;36mCollection.add\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     43\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     (\n\u001b[1;32m     76\u001b[0m         ids,\n\u001b[1;32m     77\u001b[0m         embeddings,\n\u001b[1;32m     78\u001b[0m         metadatas,\n\u001b[1;32m     79\u001b[0m         documents,\n\u001b[1;32m     80\u001b[0m         uris,\n\u001b[0;32m---> 81\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_prepare_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_add(ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, embeddings, metadatas, documents, uris)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:272\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_and_prepare_embedding_set\u001b[39m(\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    247\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m     Optional[URIs],\n\u001b[1;32m    264\u001b[0m ]:\n\u001b[1;32m    265\u001b[0m     (\n\u001b[1;32m    266\u001b[0m         ids,\n\u001b[1;32m    267\u001b[0m         embeddings,\n\u001b[1;32m    268\u001b[0m         metadatas,\n\u001b[1;32m    269\u001b[0m         documents,\n\u001b[1;32m    270\u001b[0m         images,\n\u001b[1;32m    271\u001b[0m         uris,\n\u001b[0;32m--> 272\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# We need to compute the embeddings if they're not provided\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/models/CollectionCommon.py:174\u001b[0m, in \u001b[0;36mCollectionCommon._validate_embedding_set\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris, require_embeddings_or_data)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_embedding_set\u001b[39m(\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    154\u001b[0m     ids: OneOrMany[ID],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m     Optional[URIs],\n\u001b[1;32m    173\u001b[0m ]:\n\u001b[0;32m--> 174\u001b[0m     valid_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     valid_embeddings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    176\u001b[0m         validate_embeddings(\n\u001b[1;32m    177\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_embeddings(maybe_cast_one_to_many_embedding(embeddings))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m     valid_metadatas \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    183\u001b[0m         validate_metadatas(maybe_cast_one_to_many_metadata(metadatas))\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metadatas \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/chromadb/api/types.py:273\u001b[0m, in \u001b[0;36mvalidate_ids\u001b[0;34m(ids)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(ids)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as IDs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected IDs to be a non-empty list, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m IDs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    275\u001b[0m dups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected IDs to be a non-empty list, got 0 IDs"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "if not path.exists( environ[\"_RAG_PAGE_DESTN\"] ):\n",
    "    makedirs( environ[\"_RAG_PAGE_DESTN\"] )\n",
    "\n",
    "\n",
    "def get_page_meta_key( source, page ):\n",
    "    \"\"\" Generate a (probably not) unique page key with useful data that can also be used for sorting \"\"\"\n",
    "    return str( source ).split('/')[-1].replace(' ','') + '_' + str( page )\n",
    "\n",
    "\n",
    "if int(environ[\"_RAG_DOCDB_REMAIN\"]) > 0:\n",
    "    bgn = now()\n",
    "    docIDs = [str( gen_ID() ) for _ in range( len(pages) )]\n",
    "    dcmnts = [str( pg.page_content ) for pg in pages]\n",
    "    metaDt = list()\n",
    "    d      = 50\n",
    "\n",
    "    for i, pg in enumerate( pages ):\n",
    "\n",
    "        id_i  = docIDs[i]\n",
    "        \n",
    "        # Save Text Metadata #\n",
    "        mDct = pg.metadata\n",
    "        mDct['metakey'] = get_page_meta_key( pg.metadata['source'], pg.metadata['page'] )\n",
    "        mDct['docID'  ] = id_i\n",
    "        metaDt.append( mDct )\n",
    "\n",
    "        # Save PDF Page image #\n",
    "        try:\n",
    "            pkl_i = path.join( environ[\"_RAG_PAGE_DESTN\"], f\"{id_i}.pkl\" )\n",
    "            pgPic = pdf_page_to_base64( pg.metadata['source'], pg.metadata['page'] )\n",
    "            \n",
    "            with open( pkl_i, 'wb' ) as f:\n",
    "                RAGstate['pages'][ id_i ] = str( pkl_i )\n",
    "                pickle.dump( pgPic, f )\n",
    "        except Exception as e:\n",
    "            print( f\"Could NOT save image ID {id_i}!, {e}\" )\n",
    "\n",
    "        if (i % d == 0):\n",
    "            print( '.', end='', flush = True )\n",
    "    print()\n",
    "    \n",
    "    collection.add(\n",
    "        ids       = docIDs, \n",
    "        metadatas = metaDt,\n",
    "        documents = dcmnts\n",
    "    )\n",
    "    print( f\"Added {len(dcmnts)} documents in {(now()-bgn)/60.0} minutes!\" )\n",
    "\n",
    "save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6c3459-9cfc-4e17-b249-172bce6cdf2a",
   "metadata": {},
   "source": [
    "# Create vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5000e0-162c-4f28-9f2b-1e896f42f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgn = now()\n",
    "vector_store_from_client = Chroma(\n",
    "    client             = persistent_client,\n",
    "    collection_name    = environ[\"_RAG_DOC_DBASE\"],\n",
    "    embedding_function = local_embeddings,\n",
    ")\n",
    "print( f\"Built vector store in {(now()-bgn)} seconds!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7fbc1-f2ba-4ad4-aef4-8bec59bca58d",
   "metadata": {},
   "source": [
    "# Load VLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf9a2dc-a228-43c6-bd29-42247f7c9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "pull_ollama_model( \"llava\" )\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llava\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0461630-e0ca-4dfe-924f-ef19fb70d688",
   "metadata": {},
   "source": [
    "# Setup LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c1e36-8c5d-4191-9faa-d07f579578df",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V1\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "# from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vector_store_from_client.as_retriever()\n",
    "\n",
    "# Instantiation using from_template (recommended)\n",
    "prompt1 = PromptTemplate( \n",
    "    template = \"\"\"You are an expert assistant capable of interpreting textual information to provide accurate \n",
    "                  and detailed responses. You are provided with the following data:\n",
    "                  Context: {docData}\n",
    "                  Text query: {userQuery}\n",
    "                  Use your understanding of the provided context to generate a response to based on \n",
    "                  relevant, up-to-date information. Ensure your answer is factually accurate, detailed, and leverages academic \n",
    "                  sources where possible. If additional context is required for clarification, request it from the user.\"\"\",\n",
    "    input_variables = [\"docData\",\"userQuery\"],\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain1 = (\n",
    "    { \"docData\": retriever | format_docs, \n",
    "      \"userQuery\": RunnablePassthrough()}\n",
    "    | prompt1\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855a5e1-58d6-4551-b78d-ad7b964c3e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_sources( q ):\n",
    "    retrieved_docs = retriever.invoke( q )\n",
    "    generated_ansr = rag_chain1.invoke( q )\n",
    "    return {\n",
    "        'response' : generated_ansr,\n",
    "        'sources'  : retrieved_docs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444e3c3c-c046-4e1a-9604-b8409a5e9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_local_pages( sourceList ):\n",
    "    global RAGstate\n",
    "    rtnObjs = list()\n",
    "    for source in sourceList:\n",
    "        if source.metadata['docID'] in RAGstate['pages']:\n",
    "            pklPath = RAGstate['pages'][ source.metadata['docID'] ]\n",
    "            with open( pklPath, 'rb' ) as f:\n",
    "                obj_i = pickle.load( f )\n",
    "                rtnObjs.append( obj_i )\n",
    "        else:\n",
    "            print( f\"No page with ID {source.metadata['docID']}\" )\n",
    "    return rtnObjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94141ba-2bb3-4dc1-8063-dac1ebc4c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.display import Image as IPImage\n",
    "from IPython.display import display\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def deep_doc_ask( q ):\n",
    "    bgn = now()\n",
    "    res = ask_with_sources( q ) \n",
    "    pprint( res['response'] )\n",
    "    print( f\"Initial LLM summary took {now()-bgn} seconds to process!\" )\n",
    "    \n",
    "    pag = fetch_local_pages( res['sources'] )\n",
    "\n",
    "    for p in pag:\n",
    "        display( IPImage( data = base64.b64decode( p ) ) )\n",
    "        message = HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": q},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{p}\"},\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        bgn = now()\n",
    "        response = llm.invoke( [message] )\n",
    "        print( f\"LLM query took {now()-bgn} seconds to process!\" )\n",
    "        pprint( response.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979cf56-711a-4fc9-8961-6fe1ea5dc609",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_doc_ask( \"Describe an efficient motion planning algorithm for robotic manipulation.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f180b5-5192-4a6b-bab0-056113ccfdb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49278e45-0731-427b-a86a-082e7a431e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87af5cf-40b1-4912-ae69-fd7d04565a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1815ace-9adc-4c72-aaaf-d37d6f68ea0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd0bce-e758-4e0b-a1b1-af1e361de2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7d09c-38fe-45de-8dd2-254a9914bd16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fb173-ae41-46bb-b930-8cff12c68006",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8ec26-bdc9-484e-b88d-e552b7bbee1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
