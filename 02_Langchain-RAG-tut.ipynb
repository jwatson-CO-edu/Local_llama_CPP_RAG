{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9f6fbc-8227-4e15-b6de-6d83250dac88",
   "metadata": {},
   "source": [
    "### https://python.langchain.com/docs/tutorials/local_rag/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be0190-6fad-4280-8161-c151b2709e1c",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Ollama\n",
    "* `cd /tmp`\n",
    "* `curl -fsSL https://ollama.com/install.sh | sh`\n",
    "* Test, Optional (2GB download): `ollama run llama3.2`, Type `/bye` when done\n",
    "### Install Langchain\n",
    "* `python3.10 -m pip install langchain langchain_community langchain_chroma langchain_ollama beautifulsoup4 --user`\n",
    "### Install SQLite ( >= 3.35.0 required, This will install 3.46 )\n",
    "* `sudo apt install libreadline-dev python3.10-dev`\n",
    "* `wget https://sqlite.org/2024/sqlite-autoconf-3460100.tar.gz`\n",
    "* `tar -xvf sqlite-autoconf-3460100.tar.gz && cd sqlite-autoconf-3460100`\n",
    "* `./configure`\n",
    "* `make`\n",
    "* `sudo make install`\n",
    "* `python3.10 -m pip uninstall pysqlite3`\n",
    "* `python3.10 -m pip install pysqlite3-binary --user`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6743e6a3-f373-4833-b77b-a6c00adf36bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader( \"https://lilianweng.github.io/posts/2023-06-23-agent/\" )\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede2d286-92d0-4de8-ba27-7320f2c7b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_ollama_model( modelStr ):\n",
    "    \"\"\" Pull a named model from Ollama and store it wherever \"\"\"\n",
    "    print( f\"About to save '{modelStr}'.\\nThis will spew a lot of text on the first run...\" )\n",
    "    os.system( f\"ollama pull {modelStr}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f205f6ca-0265-4ea1-bdf5-da58b719013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'nomic-embed-text'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys, os\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# from langchain_community import embeddings\n",
    "\n",
    "pull_ollama_model( \"nomic-embed-text\" )\n",
    "\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = \"nomic-embed-text\" )\n",
    "vectorstore      = Chroma.from_documents( documents = all_splits, embedding = local_embeddings )\n",
    "\n",
    "## https://stackoverflow.com/a/78164483 ##\n",
    "# persist_directory = \"/tmp/chromadb\"\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=all_splits,\n",
    "#     collection_name=\"test\",\n",
    "#     # embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text')\n",
    "#     embedding=embeddings.OllamaEmbeddings(model='nomic-embed-text')\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae10ba8-97c7-4ba1-bc03-c51b2b68fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0fc2d1-0707-4775-8dd1-97f53f07e086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60590e40-a6c7-4d42-81cf-f21198192005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'llama3.1:8b'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 8eeb52dfb3bb... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 948af2743fc7... 100% ▕████████████████▏ 1.5 KB                         \n",
      "pulling 0ba8f0e314b4... 100% ▕████████████████▏  12 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 1a4c3c319823... 100% ▕████████████████▏  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "pull_ollama_model( \"llama3.1:8b\" )\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.1:8b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c2ebe-d1f6-4fd4-b979-9f3085acfa0f",
   "metadata": {},
   "source": [
    "Graphics card is being used ...\n",
    "```\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce GTX 960         Off | 00000000:01:00.0  On |                  N/A |\n",
    "|  0%   60C    P5              19W / 128W |    433MiB /  4096MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "|   1  NVIDIA GeForce GTX 1660 Ti     Off | 00000000:02:00.0 Off |                  N/A |\n",
    "| 46%   52C    P2              79W / 120W |   4770MiB /  6144MiB |     95%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "```\n",
    "Respone took 47.18 seconds to generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3287f06b-e5ad-4656-927d-aa51fdecb4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The scene is set in a dark, crowded nightclub. The crowd is cheering as the host, Snoop Dogg, takes the stage to introduce the main event: a rap battle for the ages, featuring two of the most formidable opponents in comedy - Stephen Colbert and John Oliver!**\n",
      "\n",
      "Snoop Dogg: \"Yo, what's good everybody? Welcome to the ultimate showdown in comedic spitting. In the blue corner, we got the one and only... **Stephen Colbert**!\"\n",
      "(The crowd goes wild as Stephen Colbert steps up to the mic, dressed in his signature red, white, and blue attire.)\n",
      "\n",
      "Colbert:\n",
      "Listen up, y'all, I'm here to say\n",
      "My raps are tighter than a News Network stay\n",
      "I'm the truth-teller, the one you can't ignore\n",
      "Got my facts straight, leave no room for more\n",
      "\n",
      "**Now, in the red corner, we got the man from... England? **John Oliver**, let's get this!**\n",
      "\n",
      "Oliver:\n",
      "Hold up, Steve, I got some news to share\n",
      "Your raps are stale, like your 'Late Night Caring'\n",
      "You may have fooled the folks with your Colbert charm\n",
      "But when it comes to truth, you're just a well-paid alarm\n",
      "\n",
      "Colbert:\n",
      "Oh, so now you're dissing my style?\n",
      "Got news for you, John, my rhymes make you go wild!\n",
      "I've got the fake news facts on lock and key\n",
      "While you're over here trying to be edgy, but just being meh!\n",
      "\n",
      "Oliver:\n",
      "Edgy? Ha! You mean like your Trump-loving past?\n",
      "Trying to fit in with the cool kids, but always looking aghast\n",
      "My rants are fire, my jokes cut deep\n",
      "You may have the Daily Show, but I'm the one who makes people sleep\n",
      "\n",
      "Colbert:\n",
      "Sleep? That's rich coming from you, Mr. 'Last Week Tonight'\n",
      "Your show's a snooze-fest, where the humor's not quite tonight\n",
      "I'm the king of satire, the master of the game\n",
      "You're just a British import trying to get in on the fame!\n",
      "\n",
      "Oliver:\n",
      "Import? Hey, at least I don't wear patriotic garb like a fool\n",
      "My show may be dark, but it's where the truth is cool\n",
      "You can keep your fake smiles and 'We're number one' chants\n",
      "I'll take the criticism, because that's what matters in this rant!\n",
      "\n",
      "**The crowd goes wild as both opponents deliver their final blows! The judges (Jimmy Fallon, Trevor Noah, and Hasan Minhaj) deliberate for a moment before announcing the winner...**\n",
      "\n",
      "Snoop Dogg: \"And the winner of this epic rap battle is... **John Oliver**!\"\n",
      "(The crowd erupts in cheers as John Oliver raises his arms in triumph, while Stephen Colbert shakes hands with his opponent, showing respect for the win.)\n",
      "\n",
      "Colbert: \"Well, I guess that means I lost. But hey, at least I had fun trying to spit some fire! And to John, good job - you took it like a champ!\"\n",
      "\n",
      "Oliver: \"Thanks, Steve! You're not bad yourself. Maybe next time we can just have a rap-off and then go grab a pint together?\"\n",
      "\n",
      "(Both opponents exit the stage, arm-in-arm, as the crowd continues to cheer and chant their names.)\n",
      "Respone took 87.12658739089966 seconds to generate!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "now = time.time\n",
    "\n",
    "bgn = now()\n",
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print( response_message.content )\n",
    "print( f\"Respone took {now()-bgn} seconds to generate!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af68f9e-f52e-4448-983f-dc2ba22adf42",
   "metadata": {},
   "source": [
    "### https://python.langchain.com/docs/tutorials/local_rag/#using-in-a-chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1e227ab-e2b5-4335-a44a-96a65f606cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these documents are:\\n\\n1. **Task Decomposition**: Breaking down complex tasks into smaller, manageable subgoals using various methods such as:\\n\\t* Using Large Language Models (LLM) with simple prompts or task-specific instructions.\\n\\t* Receiving human inputs to guide the process.\\n2. **Autonomous Agent System**: An overview of a system that utilizes LLMs for autonomous decision-making and execution.\\n3. **Planning and Execution**:\\n\\t* Task planning: Breaking down tasks into subgoals, enabling efficient handling of complex tasks.\\n\\t* Task execution: Expert models execute specific tasks and log results.\\n4. **Reflection and Improvement**: The ability to reflect on past actions, learn from mistakes, and refine future steps for improved quality of final results.\\n\\nThese themes are centered around the capabilities and applications of Large Language Models (LLMs) in decision-making and task execution, with a focus on efficiency, flexibility, and continuous improvement.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert loaded documents into strings by concatenating their content\n",
    "# and ignoring metadata\n",
    "def format_docs( docs ):\n",
    "    return \"\\n\\n\".join( doc.page_content for doc in docs )\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff44e542-f35b-4d88-a4de-350fc9239cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to Task Decomposition: (1) using Large Language Models (LLM) with simple prompting, (2) applying task-specific instructions, and (3) incorporating human inputs. This process involves breaking down large tasks into smaller, manageable subgoals to enable efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1df94b4-0663-4aaa-8358-e0688bce8be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, there are three approaches to Task Decomposition: (1) using Large Language Models (LLM) with simple prompting, (2) utilizing task-specific instructions, and (3) incorporating human inputs.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke( question )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd89ca0-7267-4d07-9728-30577cee6a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2853d-bb7a-4078-b538-509784306713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9e81bb-cc68-42f2-9127-a4cf700b23f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee247b-c79e-47dd-b86f-e1c586222f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
