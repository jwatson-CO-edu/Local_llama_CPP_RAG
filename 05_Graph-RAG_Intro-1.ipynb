{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90a2fda-d974-4e87-8038-f6c5b5bf47f3",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* [AI-Scientist](https://github.com/SakanaAI/AI-Scientist )\n",
    "* [LangChain + Neo4j GRG Tutorial](https://python.langchain.com/docs/tutorials/graph/)\n",
    "* [Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09e7ba-2f31-4bf3-87b9-25cd6df751cf",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f89225-fe56-4b1b-a1da-f8a2be6cdf84",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Notebook 04 Dependencies!\n",
    "* `sudo apt install docker.io`\n",
    "* `sudo chmod 666 /var/run/docker.sock`\n",
    "* `python3.10 -m pip install docker --user`\n",
    "### Install ArangoDB + Docker Container\n",
    "* `sudo docker pull arangodb`\n",
    "* `python3.10 -m pip install python-arango adb-cloud-connector --user`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c74f26-a5d8-4b32-9fa0-83d730041ede",
   "metadata": {},
   "source": [
    "# Inspiration\n",
    "* [Co-STORM @ Stanford](https://storm.genie.stanford.edu/)\n",
    "* [Ellicit](https://elicit.com/)\n",
    "* [Research Rabbit](https://www.researchrabbit.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec290487-1b04-4c74-9f95-a1e413cda7b7",
   "metadata": {},
   "source": [
    "# Init + Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87451cf5-6fc1-4a89-8644-84f31850ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## INIT ####################################################################################\n",
    "import os\n",
    "from os import path, makedirs, environ\n",
    "from utils import copy_pdfs\n",
    "\n",
    "\n",
    "\n",
    "########## ENVIRONMENT #############################################################################\n",
    "\n",
    "##### 04: Basic RAG #######################################################\n",
    "environ[\"_RAG_DOC_DBASE\"]  = \"lit_pdf\"\n",
    "environ[\"_RAG_DOC_EMBED\"]  = \"all-minilm\"\n",
    "\n",
    "##### 05: Graph-RAG (GRG) #################################################\n",
    "environ[\"_GRG_MODEL_NAME\"] = \"llama3.2-vision\"\n",
    "environ[\"_GRG_EMBED_NAME\"] = \"all-minilm\"\n",
    "\n",
    "environ[\"_GRG_GRAPH_DB\"] = \"grg_rel\"\n",
    "\n",
    "##### Flags ###############################################################\n",
    "_LINK_PAGES = True\n",
    "\n",
    "\n",
    "##### Files ###############################################################\n",
    "_PAGE_LINKS = \"data/PageLinksDONE.txt\"\n",
    "_DOC_EMBEDS = \"data/DocVectors.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdea47-2aff-4154-80c7-9fef77992342",
   "metadata": {},
   "source": [
    "### You may need to manually tune these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003c93f9-9c84-461f-b299-790fce004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIM_MAX         =  0.7329191963152777\n",
    "_SIM_MIN         = -0.24439346017677427\n",
    "_MAX_BRANCH      = 10\n",
    "_PAGE_CSN_FRAC   =  0.60 # 1.5% of all possible links\n",
    "_PAGE_CSN_THRESH = (_SIM_MAX-_SIM_MIN) * _PAGE_CSN_FRAC + _SIM_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e6de-09de-41c3-ad97-393d3e3d58ca",
   "metadata": {},
   "source": [
    "# Depth 1: Link PDF Pages by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de0c4d-5605-4d34-a271-c502f63be5f4",
   "metadata": {},
   "source": [
    "## Retrieve 04 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62a0cbf-525b-42d0-9031-e04e8d252f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Loading the vector store sometimes spews warnings\n",
    "import sys\n",
    "\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient();\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b790070a-dbe5-4fed-b01f-8eb06dc69eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vecPairs = None\n",
    "\n",
    "if _LINK_PAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5557def-21c0-40a7-b871-ce4a6fafb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "environ[\"OLLAMA_NUM_PARALLEL\"]      = \"8\"\n",
    "environ[\"OLLAMA_MAX_LOADED_MODELS\"] = \"8\"\n",
    "\n",
    "from utils import pull_ollama_model\n",
    "\n",
    "pull_ollama_model( environ[\"_RAG_DOC_EMBED\"] )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c4b50-609c-45bc-a610-5fc5ecbeb226",
   "metadata": {},
   "source": [
    "## Recalculate Embeddings (NOT exposed by ChromaDB!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e42281-259a-4a8f-8432-138cc0fcfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you don’t have enough VRAM it will use the CPU. \n",
    "\n",
    "import time, os\n",
    "now = time.time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "\n",
    "    vecPairs = deque()\n",
    "    docBatch = 1\n",
    "\n",
    "    # Iterate over all documents and collect the IDs\n",
    "    all_ids = deque()\n",
    "    allData = collection.get()\n",
    "    totDocs = allData['documents']\n",
    "    totIDs  = allData['ids']\n",
    "    print( f\"Fetched {len(totDocs)} documents\" )\n",
    "    \n",
    "    tBgn = now()\n",
    "    N    = len( totDocs )\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    # for i, doc in enumerate( totDocs ):\n",
    "    while bgn < N:\n",
    "        end = min( bgn+docBatch, N )\n",
    "        try:\n",
    "            vec = local_embeddings.embed_documents( totDocs[ bgn:end ] )\n",
    "            for i in range( bgn, end ):\n",
    "                vecPairs.append( {'vec' : np.array( vec[i-bgn] ), 'doc': totDocs[i], 'id' : totIDs[i]} )\n",
    "                if ((i+1)%100) == 0:\n",
    "                    print('.',end='',flush=True)\n",
    "                if ((i+1)%10000) == 0:\n",
    "                    m,s = divmod( now()-tBgn, 60 )\n",
    "                    print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "            bgn = end\n",
    "        except Exception as e:\n",
    "            print(e,end=', ',flush=True)\n",
    "            bgn += 1\n",
    "    print( f\"\\nPage embedding recalc took {(now()-tBgn)/60.0:.2f} minutes!\" )\n",
    "\n",
    "    vecPairs = list( vecPairs )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1562139f-e122-4bcf-b428-89da709e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    simMin = 1e9\n",
    "    simMax = 0.0\n",
    "    print( len(vecPairs) )\n",
    "    vectrs = [item['vec'] for item in vecPairs]\n",
    "    vec0   = vectrs[0]\n",
    "    \n",
    "    for vec_i in vectrs[1:]:\n",
    "        sim_i  = cosine_similarity( [vec0, vec_i,] )[0,1]\n",
    "        simMin = min( sim_i, simMin )\n",
    "        simMax = max( sim_i, simMax )\n",
    "    \n",
    "    diffSpan = simMax - simMin\n",
    "    \n",
    "    print( [simMin, simMax,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7afa4e-5b50-46ad-8c81-988a654e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "    with open( _DOC_EMBEDS, 'wb' ) as f:\n",
    "        pickle.dump( vecPairs, f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085508e0-626b-4f3a-b321-ce024b8e8e87",
   "metadata": {},
   "source": [
    "## Calculate Page Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c952741-7fc6-46c8-9e40-76423919e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    \n",
    "    Ndocs     = len( vecPairs )\n",
    "    tBgn      = now()\n",
    "\n",
    "    Nchunk = 500\n",
    "\n",
    "    bgn1 = 0\n",
    "    end1 = 0\n",
    "    bgn2 = 0\n",
    "    end2 = 0\n",
    "\n",
    "    def attempt_connect( pair_i, pair_j ):\n",
    "        sim_ij = cosine_similarity( [pair_i['vec'], pair_j['vec'],] )[0,1]\n",
    "        if sim_ij >= _PAGE_CSN_THRESH:\n",
    "            return {\n",
    "                'type'   : \"Page_Cosine_Similarity\",\n",
    "                'idTail' : pair_i['id'],\n",
    "                'idHead' : pair_j['id'],\n",
    "                'dir'    : False,\n",
    "                'coSim'  : sim_ij,\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    while bgn1 < Ndocs:\n",
    "\n",
    "        pageLinks = deque()\n",
    "\n",
    "        ## Define Chunks 1 & 2 ##\n",
    "        end1 = min( bgn1+Nchunk, Ndocs )\n",
    "        bgn2 = end1 if (end1 < Ndocs) else 0 # Chunk 2 wraps at the end\n",
    "        end2 = min( bgn2+Nchunk, Ndocs )\n",
    "\n",
    "        chnk1 = vecPairs[ bgn1:end1 ]\n",
    "        chnk2 = vecPairs[ bgn2:end2 ]\n",
    "\n",
    "        N1 = end1 - bgn1\n",
    "        N2 = end2 - bgn2\n",
    "\n",
    "        ## Connections between Chunks 1 & 2 ##\n",
    "        for i in range( N1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('.',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( N2 ):\n",
    "                pair_j = chnk2[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        ## Connections within Chunk 1 ##\n",
    "        for i in range( N1-1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('~',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( i+1, N1 ):\n",
    "                pair_j = chnk1[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        pageLinks = list( pageLinks )\n",
    "        with open( f\"data/PageLinks_{bgn1}-{end2}.pkl\", 'wb' ) as f:\n",
    "            pickle.dump( pageLinks, f )\n",
    "        \n",
    "        m,s = divmod( now()-tBgn, 60 )\n",
    "        print(f\"\\n{bgn1}:{end1}/{bgn2}:{end2}, {int(m)}:{s:.2f}, {len(pageLinks)},\",end=' ',flush=True)\n",
    "        bgn1 = end1\n",
    "\n",
    "    os.system( f\"touch {_PAGE_LINKS}\" )\n",
    "    print( f\"\\nBuilt page graph in {(now()-tBgn)/60.0/60.0:.2f} hours!\" )\n",
    "    print()\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133da01-c15b-4427-ac1b-815510e2960b",
   "metadata": {},
   "source": [
    "## Build Page Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724cfa6-6819-4f76-a9fd-5efd1d813438",
   "metadata": {},
   "source": [
    "### Build Page Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ba23d-af9c-40cf-83e5-643b79671f6a",
   "metadata": {},
   "source": [
    "### Build Page Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5117a2e-6a1d-4f3c-9258-6a5da33c31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f\"There are {len(pageLinks)} connections between pages!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0337e-e003-45c0-9a5d-ca042a5191cc",
   "metadata": {},
   "source": [
    "# Depth 2: Link Passages by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e83c19-60b7-4fb1-ba5b-5d737ad0d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "_GEN_PASSAGES = True\n",
    "_PASSAGE_FNAM = \"data/Passages.pkl\"\n",
    "_PSG_DIST_DIV = 1.41 # Lower number, Few Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f569ef3-fecf-4c82-be8c-181546e32fe7",
   "metadata": {},
   "source": [
    "## Segment Pages Into Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4949e3d5-163d-4366-848a-fbb2d82b1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def page_to_sentences( pageText ):\n",
    "    \"\"\" Parse the page into individual sentences \"\"\"\n",
    "    rtnParts = deque()\n",
    "    pageText = fr\"{pageText}\" + '.' # Terminator hack\n",
    "    sepChars = ['?','!','\\n']\n",
    "    sepPhras = ['. ',]\n",
    "    sentence = \"\"\n",
    "    word     = \"\"\n",
    "\n",
    "    # print( len(pageText) )\n",
    "    # pprint( pageText )\n",
    "\n",
    "    def push_chunk():\n",
    "        nonlocal rtnParts, sentence, word\n",
    "        rtnParts.append( sentence )\n",
    "        sentence = \"\"\n",
    "        word     = \"\"\n",
    "    \n",
    "    for c in pageText:\n",
    "        sentence += c # Include punctuation in the sentence\n",
    "        word     += c\n",
    "        if (c in sepChars) or (word in sepPhras):\n",
    "            push_chunk()\n",
    "\n",
    "    chunks = list( rtnParts )\n",
    "    chunks = [str( part ).strip() for part in chunks]\n",
    "\n",
    "    # print( chunks )\n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9958dc-c23b-4003-b0e4-2108294715ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_ID\n",
    "\n",
    "def sentences_to_passages( chunkList, embedder, parentID, segDiv = 10.0 ):\n",
    "    \"\"\" Segment a list of sentences into a passage \"\"\"\n",
    "    vectors = embedder.embed_documents( chunkList )\n",
    "    senVecs = zip( vectors, chunkList )\n",
    "    rtnSeg  = deque()\n",
    "\n",
    "    def vec_diff( v1, v2 ):\n",
    "        \"\"\" Distance between 2 pnts \"\"\"\n",
    "        return np.linalg.norm( np.subtract( v1, v2 ) )\n",
    "\n",
    "    def get_total_width():\n",
    "        nonlocal vectors\n",
    "        N    = len( vectors )\n",
    "        dMax = -1.0\n",
    "        for i in range( N-1 ):\n",
    "            for j in range( i+1, N ):\n",
    "                dMax = max( dMax, vec_diff( vectors[i], vectors[j] ) )\n",
    "        return dMax\n",
    "\n",
    "    segRad = get_total_width() / segDiv\n",
    "    \n",
    "    vecs_j = list()\n",
    "    mean_j = None\n",
    "    txtP_j = \"\"\n",
    "\n",
    "    for i, (vec_i, sen_i) in enumerate( senVecs ):\n",
    "\n",
    "        if i == 0:\n",
    "            mean_j = vec_i\n",
    "\n",
    "        if vec_diff( vec_i, mean_j ) > segRad:\n",
    "            rtnSeg.append({\n",
    "                'id'    : gen_ID(),\n",
    "                'vec'   : embedder.embed_documents( [txtP_j,] )[0], # Embedding might be different than the average?\n",
    "                'txt'   : txtP_j,\n",
    "                'pageID': str( parentID ),\n",
    "            })\n",
    "            vecs_j = [vec_i,]\n",
    "            mean_j = vec_i\n",
    "            txtP_j = sen_i\n",
    "        else:\n",
    "            vecs_j.append( vec_i )\n",
    "            mean_j = np.mean( vecs_j, axis = 0 )\n",
    "            txtP_j += ' ' + sen_i # Reinsert leading space\n",
    "\n",
    "    return list( rtnSeg )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032de2a1-522d-4884-a44f-ca353de25612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "vecPairs = list()\n",
    "if _GEN_PASSAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d44168e-34d6-4043-a6c1-60caf1f06930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# 2024-11-11: THIS IS A 30 DAY CALCULATION!!!  3.6 HOURS PER 10k PAGES!  RUN JOBS AS AVAILABLE!\n",
    "# 2024-11-12: Various sources claim Ollama handles parallel processing?  Is it thread safe?  ATTEMPT AND SEE!!\n",
    "\n",
    "def embed_large_passage_collection( manyVecPairs, fragmentID = None ):\n",
    "    \"\"\" Attempt to embed a large collection of documents \"\"\"\n",
    "    # NOTE: Collection should be large enough to justify loading a new instance of the embedding model!\n",
    "    print( f\"\\nBegin processing passages @ {os.getpid()}!\" )\n",
    "    lclEmbd  = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )\n",
    "    passages = deque()\n",
    "    tBgn     = now()\n",
    "\n",
    "    if fragmentID is None:\n",
    "        fragmentID = gen_ID()\n",
    "    \n",
    "    for i, pair_i in enumerate ( manyVecPairs ):\n",
    "        try:\n",
    "            dID_i = pair_i['id' ]\n",
    "            doc_i = pair_i['doc']\n",
    "            vec_i = pair_i['vec']\n",
    "\n",
    "            chunks_i = page_to_sentences( doc_i )\n",
    "            passgs_i = sentences_to_passages( chunks_i, lclEmbd, dID_i, segDiv = _PSG_DIST_DIV ) \n",
    "            passages.extend( passgs_i )\n",
    "\n",
    "        except Exception as e:\n",
    "            print( f\"\\nBAD THING @ {os.getpid()}: {e}\\n\" )\n",
    "            \n",
    "    print( f\"\\nFINISHED processing passages @ {os.getpid()}!\" )\n",
    "    return list( passages )\n",
    "\n",
    "# FIXME, START HERE: NEED A POOL OF WORKERS\n",
    "\n",
    "# FIXME: NEED A SEMAPHOR FOR WRITING TO RAG STATE?\n",
    "\n",
    "# FIXME: IS THERE A WAY TO STOP PROCESSING AND STILL PRESERVE STATE?\n",
    "\n",
    "# FIXME: PICK UP FROM LAST 10k INDEX @ RAGSTATE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793761a-46ae-48f6-9466-c695a0a30da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528eb7f-282b-4fc8-80c7-119437d0a6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52a757-4438-4d51-bbb9-e512221330c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f93215-60bf-4a76-b59f-e9b67799151d",
   "metadata": {},
   "source": [
    "## Link Passages to Parent Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5591b7-079f-43b2-abba-0c1f31c89951",
   "metadata": {},
   "source": [
    "## For each passage, Gather pages from N hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eb993-7aed-4f25-a61a-4e0a7d861e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in collection.get()['documents']:\n",
    "# # for result in collection.get()['ids']: # 6f83d661-6cdf-46bf-83a4-27f6c36d948f\n",
    "#     # print( result )\n",
    "#     print( dir( result ) )\n",
    "#     break\n",
    "#     # for res in result['ids']:\n",
    "#     #     all_ids.append( res )\n",
    "# # docIDs = list( all_ids )\n",
    "# # print( f\"There are {len(docIDs)} documents!\" )\n",
    "# # print( docIDs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667b5d-ac67-4499-b9e6-c9d8f1b08f28",
   "metadata": {},
   "source": [
    "## Calc ranked passage similarity from those pages, Create up to M connections per passage\n",
    "### (This is a separate collection from page links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4408b94-4953-44fd-aa98-fb31e7bc44af",
   "metadata": {},
   "source": [
    "## Build Passage Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93294997-4586-4cef-918c-b9223d64c839",
   "metadata": {},
   "source": [
    "### Build Passage Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d881b33-1fbe-4c5b-adae-915db03ce34b",
   "metadata": {},
   "source": [
    "### Build Passage Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748be8-aa1b-404b-8efd-b8a7ba1b0a57",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f70d6-f90d-416a-b6d1-7e427f17171b",
   "metadata": {},
   "source": [
    "## What decisions does the retriever have to make?\n",
    "* Following connections\n",
    "* Ranking pages and passages\n",
    "* Stitching passages in proper order.\n",
    "    - What is proper order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dc867-db4b-4f3b-b654-1e735841ad30",
   "metadata": {},
   "source": [
    "## What is our token budget for the LLM summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8de1f-c3d2-4446-8d72-ddc6c61c8289",
   "metadata": {},
   "source": [
    "## What would it look like to extend the budget with overlapping summaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d349b-dd4d-4935-9e76-54256f8720de",
   "metadata": {},
   "source": [
    "# Create Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5804d7-6971-41b3-b303-78c3b21ab5da",
   "metadata": {},
   "source": [
    "## What would it look like for the summarizer to extend the token budget with Chain of Thought \"Reasoning\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e304759-e7fc-4735-ba76-36ed434e666c",
   "metadata": {},
   "source": [
    "# Depth 3: Build Statements (Assumptions and Claims) about Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905b2d8-6ba3-41c3-b1fd-a2f6b14c35a4",
   "metadata": {},
   "source": [
    "## Segment Keywords\n",
    "* Statistically important/rare phrases\n",
    "* Ask LLM to isolate {jargon, technical terms}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c77203-fff8-4db0-b1a3-5298ee1bf3ed",
   "metadata": {},
   "source": [
    "## Build Statements (Assumptions and Claims)\n",
    "* S-V-O sentence-by-sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd1625-3ff4-4a0e-bbb6-a3755650b554",
   "metadata": {},
   "source": [
    "# Depth 4: Support/Refute Statements (Assumptions and Claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193e0d7-3037-4eba-ab08-e2b574f4256d",
   "metadata": {},
   "source": [
    "## How to determine support?  How to determine contradiction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc56c5-df26-4305-997d-25950bcfaf88",
   "metadata": {},
   "source": [
    "## How to weigh support/contradiction based on our level of trust in existing documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe82a-21c1-410e-8757-11a0b5638820",
   "metadata": {},
   "source": [
    "# Advanced Knowledge Graph Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaf6d7-3549-40da-b28a-084eaf4b630b",
   "metadata": {},
   "source": [
    "## Statements: What is the difference between an Assumption and a Claim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6ca8f-a0b6-4abf-83a7-e81e5e22bbc2",
   "metadata": {},
   "source": [
    "## What assumptions do we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43bb88-9a67-4b51-8aac-a05aa43c2a43",
   "metadata": {},
   "source": [
    "## What claims are supported by assumptions we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f335b0-9f35-4cfd-b5ad-1c64252c7e91",
   "metadata": {},
   "source": [
    "## What questions are being asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b235e-2b90-4297-9ce0-c941a89c58c1",
   "metadata": {},
   "source": [
    "## What questions are being answered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0552f-3c89-4de4-9da9-a087489a16b2",
   "metadata": {},
   "source": [
    "## Does the document create new trustworthy connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3409c-8f9c-4a75-983f-5d4ab6c1a37b",
   "metadata": {},
   "source": [
    "## Can we find a similar subgraph in a different field of research?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286e3d5-056e-44af-bf9e-2d06941da2c9",
   "metadata": {},
   "source": [
    "## What questions CAN be asked based on the movement of passages and concepts through vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a9556-3072-4def-9292-31ff04f064b9",
   "metadata": {},
   "source": [
    "## Can we track trajectories in vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b0a23-d4c6-4aa6-8e44-8957200a58d5",
   "metadata": {},
   "source": [
    "## Can we PREDICT trajectories in vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e4882-5e86-4a22-905e-e4917c82e413",
   "metadata": {},
   "source": [
    "# KNNEST: Knowledge Graph Structure Notes\n",
    "* How to know sources are in different fields? ~ Cluster embeddings?\n",
    "* New heirarchical embedding per field?\n",
    "* Do embeddings need to be compressed by PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823f6b4-3876-4c77-995b-9de8c4419ecd",
   "metadata": {},
   "source": [
    "# ANTs: Search Agent Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acd01b-9797-4d5a-a9b4-f80985a4076e",
   "metadata": {},
   "source": [
    "## Can we TRAIN an agent to make advantageous traversals on the KG based on vector space deltas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d1b4b-db2e-43d7-bcf0-074a14e67167",
   "metadata": {},
   "source": [
    "## Swarm-Level Load Management\n",
    "* Agent Instantiation Condition(s)\n",
    "* Agent Deletion Condition(s)\n",
    "* Task Start Condition(s)\n",
    "* Task Stop Condition(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71fee0-1a30-4438-8049-75b49e2f0b75",
   "metadata": {},
   "source": [
    "## ANT Decision-Making Architecture\n",
    "* Resource alotment: {Time, Compute}\n",
    "* Critical: What edge to follow?\n",
    "* A strong line of reasoning can be a demonstration trajectory? LfD?\n",
    "* Inverse-RL to produce an evaluation function for trajectories thru the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22eaf-d4bf-45b4-bfae-3e54d4d12d97",
   "metadata": {},
   "source": [
    "# Connect to Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a31a3-ee34-4a8a-8fe7-0a5669e5a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# def start_arango_container():\n",
    "#     command = [ \"docker\", \"run\", \"-p\", \n",
    "#                 \"8529:8529\", \"-e\", \"ARANGO_ROOT_PASSWORD=\", \"arangodb/arangodb\"]  \n",
    "#     subprocess.Popen( command )\n",
    "\n",
    "# start_arango_container()\n",
    "# sleep( 15.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c39f5-d011-4487-8984-9869c3d7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate ArangoDB Database\n",
    "# import json\n",
    "\n",
    "# from adb_cloud_connector import get_temp_credentials\n",
    "# from arango import ArangoClient\n",
    "\n",
    "# con = get_temp_credentials()\n",
    "\n",
    "# db = ArangoClient(hosts=con[\"url\"]).db(\n",
    "#     con[\"dbName\"], con[\"username\"], con[\"password\"], verify=True\n",
    "# )\n",
    "\n",
    "# print(json.dumps(con, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7e008-5ddc-4bde-b666-cf6e9e8b9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the ArangoDB-LangChain Graph\n",
    "# from langchain_community.graphs import ArangoGraph\n",
    "\n",
    "# graph = ArangoGraph( db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94314a6a-941b-497b-941a-d95595615e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not db.has_graph( environ[\"_GRG_GRAPH_DB\"] ):\n",
    "#     db.create_graph(\n",
    "#         environ[\"_GRG_GRAPH_DB\"],\n",
    "#         edge_definitions=[\n",
    "#             {\n",
    "#                 \"from_vertex_collections\": [\"subjects\"],\n",
    "#                 \"edge_collection\": \"verbs\",\n",
    "#                 \"to_vertex_collections\": [\"subjects\"],\n",
    "#             },\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70a3c7-ce94-41b8-9e1c-23a2fdf1761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import docker\n",
    "\n",
    "# client     = docker.from_env()\n",
    "# containers = client.containers.list()\n",
    "\n",
    "# for container in containers:\n",
    "#     print(container.name, container.short_id, container.status)\n",
    "\n",
    "# os.system( \"docker stop 8ed55910d8cc\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39eb5c6-6c53-4c30-a1e3-80ce01551f6c",
   "metadata": {},
   "source": [
    "# Wouldn't it be cool if ...\n",
    "* A manifold (high-order hypersurface) could be fit to the motion of human knowledge\n",
    "* We could trace paths on that manifold\n",
    "* Given a position and curvature (+jerk+snap+crackle+pop), we can extrapolate motion beyond the edge of the mapped manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799705-7295-4731-b6a8-ee7ba7dacabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
