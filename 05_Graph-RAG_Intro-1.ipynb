{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90a2fda-d974-4e87-8038-f6c5b5bf47f3",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* [AI-Scientist](https://github.com/SakanaAI/AI-Scientist )\n",
    "* [LangChain + Neo4j GRG Tutorial](https://python.langchain.com/docs/tutorials/graph/)\n",
    "* [Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09e7ba-2f31-4bf3-87b9-25cd6df751cf",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f89225-fe56-4b1b-a1da-f8a2be6cdf84",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Notebook 04 Dependencies!\n",
    "* `sudo apt install docker.io`\n",
    "* `sudo chmod 666 /var/run/docker.sock`\n",
    "* `python3.10 -m pip install docker --user`\n",
    "### Install ArangoDB + Docker Container\n",
    "* `sudo docker pull arangodb`\n",
    "* `python3.10 -m pip install python-arango adb-cloud-connector --user`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c74f26-a5d8-4b32-9fa0-83d730041ede",
   "metadata": {},
   "source": [
    "# Inspiration\n",
    "* [Co-STORM @ Stanford](https://storm.genie.stanford.edu/)\n",
    "* [Ellicit](https://elicit.com/)\n",
    "* [Research Rabbit](https://www.researchrabbit.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec290487-1b04-4c74-9f95-a1e413cda7b7",
   "metadata": {},
   "source": [
    "# Init + Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87451cf5-6fc1-4a89-8644-84f31850ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## INIT ####################################################################################\n",
    "import os\n",
    "from os import path, makedirs, environ\n",
    "from utils import copy_pdfs\n",
    "\n",
    "\n",
    "\n",
    "########## ENVIRONMENT #############################################################################\n",
    "\n",
    "##### 04: Basic RAG #######################################################\n",
    "environ[\"_RAG_DOC_DBASE\" ] = \"lit_pdf\"\n",
    "environ[\"_RAG_DOC_EMBED\" ] = \"all-minilm\"\n",
    "environ[\"_RAG_STATE_PATH\"] = \"data/state.json\"\n",
    "\n",
    "##### 05: Graph-RAG (GRG) #################################################\n",
    "environ[\"_GRG_MODEL_NAME\"] = \"llama3.2-vision\"\n",
    "environ[\"_GRG_EMBED_NAME\"] = \"all-minilm\"\n",
    "\n",
    "environ[\"_GRG_GRAPH_DB\"] = \"grg_rel\"\n",
    "\n",
    "##### Flags ###############################################################\n",
    "_LINK_PAGES = True\n",
    "\n",
    "\n",
    "##### Files ###############################################################\n",
    "_PAGE_LINKS = \"data/PageLinksDONE.txt\"\n",
    "_DOC_EMBEDS = \"data/DocVectors.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdea47-2aff-4154-80c7-9fef77992342",
   "metadata": {},
   "source": [
    "### You may need to manually tune these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003c93f9-9c84-461f-b299-790fce004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIM_MAX         =  0.7329191963152777\n",
    "_SIM_MIN         = -0.24439346017677427\n",
    "_MAX_BRANCH      = 10\n",
    "_PAGE_CSN_FRAC   =  0.60 # 1.5% of all possible links\n",
    "_PAGE_CSN_THRESH = (_SIM_MAX-_SIM_MIN) * _PAGE_CSN_FRAC + _SIM_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e6de-09de-41c3-ad97-393d3e3d58ca",
   "metadata": {},
   "source": [
    "# Depth 1: Link PDF Pages by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de0c4d-5605-4d34-a271-c502f63be5f4",
   "metadata": {},
   "source": [
    "## Retrieve 04 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62a0cbf-525b-42d0-9031-e04e8d252f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Loading the vector store sometimes spews warnings\n",
    "import sys\n",
    "\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient();\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b790070a-dbe5-4fed-b01f-8eb06dc69eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vecPairs = None\n",
    "\n",
    "if _LINK_PAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5557def-21c0-40a7-b871-ce4a6fafb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from utils import pull_ollama_model\n",
    "\n",
    "pull_ollama_model( environ[\"_RAG_DOC_EMBED\"] )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c4b50-609c-45bc-a610-5fc5ecbeb226",
   "metadata": {},
   "source": [
    "## Recalculate Embeddings (NOT exposed by ChromaDB!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e42281-259a-4a8f-8432-138cc0fcfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you don’t have enough VRAM it will use the CPU. \n",
    "\n",
    "import time, os\n",
    "now = time.time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "\n",
    "    vecPairs = deque()\n",
    "    docBatch = 1\n",
    "\n",
    "    # Iterate over all documents and collect the IDs\n",
    "    all_ids = deque()\n",
    "    allData = collection.get()\n",
    "    totDocs = allData['documents']\n",
    "    totIDs  = allData['ids']\n",
    "    print( f\"Fetched {len(totDocs)} documents\" )\n",
    "    \n",
    "    tBgn = now()\n",
    "    N    = len( totDocs )\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    # for i, doc in enumerate( totDocs ):\n",
    "    while bgn < N:\n",
    "        end = min( bgn+docBatch, N )\n",
    "        try:\n",
    "            vec = local_embeddings.embed_documents( totDocs[ bgn:end ] )\n",
    "            for i in range( bgn, end ):\n",
    "                vecPairs.append( {'vec' : np.array( vec[i-bgn] ), 'doc': totDocs[i], 'id' : totIDs[i]} )\n",
    "                if ((i+1)%100) == 0:\n",
    "                    print('.',end='',flush=True)\n",
    "                if ((i+1)%10000) == 0:\n",
    "                    m,s = divmod( now()-tBgn, 60 )\n",
    "                    print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "            bgn = end\n",
    "        except Exception as e:\n",
    "            print(e,end=', ',flush=True)\n",
    "            bgn += 1\n",
    "    print( f\"\\nPage embedding recalc took {(now()-tBgn)/60.0:.2f} minutes!\" )\n",
    "\n",
    "    vecPairs = list( vecPairs )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1562139f-e122-4bcf-b428-89da709e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    simMin = 1e9\n",
    "    simMax = 0.0\n",
    "    print( len(vecPairs) )\n",
    "    vectrs = [item['vec'] for item in vecPairs]\n",
    "    vec0   = vectrs[0]\n",
    "    \n",
    "    for vec_i in vectrs[1:]:\n",
    "        sim_i  = cosine_similarity( [vec0, vec_i,] )[0,1]\n",
    "        simMin = min( sim_i, simMin )\n",
    "        simMax = max( sim_i, simMax )\n",
    "    \n",
    "    diffSpan = simMax - simMin\n",
    "    \n",
    "    print( [simMin, simMax,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7afa4e-5b50-46ad-8c81-988a654e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "    with open( _DOC_EMBEDS, 'wb' ) as f:\n",
    "        pickle.dump( vecPairs, f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085508e0-626b-4f3a-b321-ce024b8e8e87",
   "metadata": {},
   "source": [
    "## Calculate Page Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c952741-7fc6-46c8-9e40-76423919e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    \n",
    "    Ndocs     = len( vecPairs )\n",
    "    tBgn      = now()\n",
    "\n",
    "    Nchunk = 500\n",
    "\n",
    "    bgn1 = 0\n",
    "    end1 = 0\n",
    "    bgn2 = 0\n",
    "    end2 = 0\n",
    "\n",
    "    def attempt_connect( pair_i, pair_j ):\n",
    "        sim_ij = cosine_similarity( [pair_i['vec'], pair_j['vec'],] )[0,1]\n",
    "        if sim_ij >= _PAGE_CSN_THRESH:\n",
    "            return {\n",
    "                'type'   : \"Page_Cosine_Similarity\",\n",
    "                'idTail' : pair_i['id'],\n",
    "                'idHead' : pair_j['id'],\n",
    "                'dir'    : False,\n",
    "                'coSim'  : sim_ij,\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    while bgn1 < Ndocs:\n",
    "\n",
    "        pageLinks = deque()\n",
    "\n",
    "        ## Define Chunks 1 & 2 ##\n",
    "        end1 = min( bgn1+Nchunk, Ndocs )\n",
    "        bgn2 = end1 if (end1 < Ndocs) else 0 # Chunk 2 wraps at the end\n",
    "        end2 = min( bgn2+Nchunk, Ndocs )\n",
    "\n",
    "        chnk1 = vecPairs[ bgn1:end1 ]\n",
    "        chnk2 = vecPairs[ bgn2:end2 ]\n",
    "\n",
    "        N1 = end1 - bgn1\n",
    "        N2 = end2 - bgn2\n",
    "\n",
    "        ## Connections between Chunks 1 & 2 ##\n",
    "        for i in range( N1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('.',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( N2 ):\n",
    "                pair_j = chnk2[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        ## Connections within Chunk 1 ##\n",
    "        for i in range( N1-1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('~',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( i+1, N1 ):\n",
    "                pair_j = chnk1[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        pageLinks = list( pageLinks )\n",
    "        with open( f\"data/PageLinks_{bgn1}-{end2}.pkl\", 'wb' ) as f:\n",
    "            pickle.dump( pageLinks, f )\n",
    "        \n",
    "        m,s = divmod( now()-tBgn, 60 )\n",
    "        print(f\"\\n{bgn1}:{end1}/{bgn2}:{end2}, {int(m)}:{s:.2f}, {len(pageLinks)},\",end=' ',flush=True)\n",
    "        bgn1 = end1\n",
    "\n",
    "    os.system( f\"touch {_PAGE_LINKS}\" )\n",
    "    print( f\"\\nBuilt page graph in {(now()-tBgn)/60.0/60.0:.2f} hours!\" )\n",
    "    print()\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133da01-c15b-4427-ac1b-815510e2960b",
   "metadata": {},
   "source": [
    "## Build Page Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724cfa6-6819-4f76-a9fd-5efd1d813438",
   "metadata": {},
   "source": [
    "### Build Page Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ba23d-af9c-40cf-83e5-643b79671f6a",
   "metadata": {},
   "source": [
    "### Build Page Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5117a2e-6a1d-4f3c-9258-6a5da33c31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f\"There are {len(pageLinks)} connections between pages!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0337e-e003-45c0-9a5d-ca042a5191cc",
   "metadata": {},
   "source": [
    "# Depth 2: Link Passages by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e83c19-60b7-4fb1-ba5b-5d737ad0d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "_GEN_PASSAGES = True\n",
    "_PASSAGE_FNAM = \"data/Passages.pkl\"\n",
    "_PSG_DIST_DIV =  1.35 # Lower number, Few Segments\n",
    "# _PSG_DIST_DIV =  1.41 # 2024-11-15: Cuts off sentences. Fragments that don't make sense on their own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fe63ce-347b-410c-a036-a9eedb101d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "environ[\"OLLAMA_NUM_PARALLEL\"]      =  \"16\"\n",
    "environ[\"OLLAMA_MAX_LOADED_MODELS\"] =   \"4\"\n",
    "environ[\"_GRG_PASSAGE_N_WORKERS\"]   =  \"16\"\n",
    "# environ[\"_GRG_PASSAGE_N_PAGES\"]     = \"160\" # About 20 mins for 16 workers, Low for TESTING\n",
    "environ[\"_GRG_PASSAGE_N_PAGES\" ]     = \"640\" # About 80 mins for 16 workers?, 300+ PKL files\n",
    "# environ[\"_GRG_PASSAGE_MIN_CHAR\"]     = \"150\" # I feel like I am skipping inportant info\n",
    "# environ[\"_GRG_PASSAGE_MIN_CHAR\"]     = \"100\" # Short and nonsensical\n",
    "environ[\"_GRG_PASSAGE_MIN_CHAR\"]     = \"200\" # Avoid embedding uselessly short passages\n",
    "\n",
    "\"\"\"\n",
    "PAR/MO/WR/PGS/FLT/SPLT:            | 200k pgs (Round-the-Clock)\n",
    "-----------------------------------|--------\n",
    " 32/ 8/ 4/ 80/NOT/1.41: ~12 min    / 20 days\n",
    " 64/16/ 8/ 80/NOT/1.41: ~11 min    / 19 days\n",
    " 32/ 8/ 8/ 80/NOT/1.41: ~11 min    / 19 days\n",
    " 16/ 4/ 8/ 80/NOT/1.41: ~11 min    / 19 days\n",
    "128/32/ 8/ 80/NOT/1.41: ~11 min    / 19 days\n",
    " 16/ 4/16/160/NOT/1.41: ~20 min    / 17 days \n",
    " 16/ 4/16/640/100/1.41:  40-50 min /  8-10 days, Cuts off sentences, Fragments don't make sense on their own\n",
    " 16/ 4/16/640/100/1.35:  40-50 min /  8-10 days, Short and nonsensical\n",
    " 16/ 4/16/640/150/1.41:  30-50 min /  7-10 days, 1/14th of the size of the non-filtered data\n",
    " 16/ 4/16/640/200/1.35:  40-50 min /  8-10 days, *** 10x smallest, 1/3 of initial, Probably a good balance?\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f569ef3-fecf-4c82-be8c-181546e32fe7",
   "metadata": {},
   "source": [
    "## Segment Pages Into Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4949e3d5-163d-4366-848a-fbb2d82b1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def page_to_sentences( pageText ):\n",
    "    \"\"\" Parse the page into individual sentences \"\"\"\n",
    "    rtnParts = deque()\n",
    "    pageText = fr\"{pageText}\" + '.' # Terminator hack\n",
    "    sepChars = ['?','!','\\n']\n",
    "    sepPhras = ['. ',]\n",
    "    sentence = \"\"\n",
    "    word     = \"\"\n",
    "\n",
    "    # print( len(pageText) )\n",
    "    # pprint( pageText )\n",
    "\n",
    "    def push_chunk():\n",
    "        nonlocal rtnParts, sentence, word\n",
    "        rtnParts.append( sentence )\n",
    "        sentence = \"\"\n",
    "        word     = \"\"\n",
    "    \n",
    "    for c in pageText:\n",
    "        sentence += c # Include punctuation in the sentence\n",
    "        word     += c\n",
    "        if (c in sepChars) or (word in sepPhras):\n",
    "            push_chunk()\n",
    "\n",
    "    chunks = list( rtnParts )\n",
    "    chunks = [str( part ).strip() for part in chunks]\n",
    "\n",
    "    # print( chunks )\n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d9958dc-c23b-4003-b0e4-2108294715ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_ID\n",
    "\n",
    "def sentences_to_passages( chunkList, embedder, parentID, segDiv = 10.0 ):\n",
    "    \"\"\" Segment a list of sentences into a passage \"\"\"\n",
    "    vectors = embedder.embed_documents( chunkList )\n",
    "    senVecs = zip( vectors, chunkList )\n",
    "    rtnSeg  = deque()\n",
    "\n",
    "    def vec_diff( v1, v2 ):\n",
    "        \"\"\" Distance between 2 pnts \"\"\"\n",
    "        return np.linalg.norm( np.subtract( v1, v2 ) )\n",
    "\n",
    "    def get_total_width():\n",
    "        nonlocal vectors\n",
    "        N    = len( vectors )\n",
    "        dMax = -1.0\n",
    "        for i in range( N-1 ):\n",
    "            for j in range( i+1, N ):\n",
    "                dMax = max( dMax, vec_diff( vectors[i], vectors[j] ) )\n",
    "        return dMax\n",
    "\n",
    "    segRad = get_total_width() / segDiv\n",
    "    \n",
    "    vecs_j    = list()\n",
    "    mean_j    = None\n",
    "    txtP_j    = \"\"\n",
    "    Nvctrs    = len( vectors )\n",
    "    minChrLen = int( environ[\"_GRG_PASSAGE_MIN_CHAR\"] )\n",
    "\n",
    "    for i, (vec_i, sen_i) in enumerate( senVecs ):\n",
    "\n",
    "        if i == 0:\n",
    "            mean_j = vec_i\n",
    "        \n",
    "        # 1. Clean up passage remainder if the page has ended\n",
    "        # -OR-\n",
    "        # 2. Require a minimum semantic distance to end a passage (discounted by length beyond minimum)\n",
    "        # -AND-\n",
    "        # 3. Require a minimum string length for a passage\n",
    "        # -OTHERWISE-\n",
    "        # 4. Accumulate semantically-relevant sentence onto the passage\n",
    "\n",
    "        discount = min( 1.0, np.exp( -1.0*(len( txtP_j ) - minChrLen) / minChrLen ) )\n",
    "        \n",
    "        if ((i+1) >= Nvctrs):\n",
    "            txtP_j += ' ' + sen_i # Reinsert leading space\n",
    "            rtnSeg.append({\n",
    "                'id'    : gen_ID(),\n",
    "                'vec'   : embedder.embed_documents( [txtP_j,] )[0], # Embedding might be different than the average?\n",
    "                'txt'   : txtP_j,\n",
    "                'pageID': str( parentID ),\n",
    "            })\n",
    "        elif (vec_diff( vec_i, mean_j ) > (segRad*discount)) and (len( txtP_j ) >= int( environ[\"_GRG_PASSAGE_MIN_CHAR\"] )):\n",
    "            rtnSeg.append({\n",
    "                'id'    : gen_ID(),\n",
    "                'vec'   : embedder.embed_documents( [txtP_j,] )[0], # Embedding might be different than the average?\n",
    "                'txt'   : txtP_j,\n",
    "                'pageID': str( parentID ),\n",
    "            })\n",
    "            vecs_j = [vec_i,]\n",
    "            mean_j = vec_i\n",
    "            txtP_j = sen_i\n",
    "        else:\n",
    "            vecs_j.append( vec_i )\n",
    "            mean_j = np.mean( vecs_j, axis = 0 )\n",
    "            txtP_j += ' ' + sen_i # Reinsert leading space\n",
    "\n",
    "    return list( rtnSeg )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "032de2a1-522d-4884-a44f-ca353de25612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "vecPairs = list()\n",
    "if _GEN_PASSAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "295ad5ee-f325-459d-98a3-1ab4ad76b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from typing import List, Dict\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "from random import choice\n",
    "\n",
    "# # Initialize the embedding model globally, as each process will need access to it\n",
    "# embedding_model = OllamaEmbeddings(model_name=\"all-MiniLM\")  # Configure model if needed\n",
    "\n",
    "# Define the worker function\n",
    "def passage_process_worker(doc_queue: multiprocessing.Queue, results: List[Dict], lock: multiprocessing.Lock):\n",
    "    \n",
    "    # print( f\"\\nBegin processing passages @ {os.getpid()}!\" )\n",
    "    lclEmbd  = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )\n",
    "    tBgn     = 0.0 \n",
    "\n",
    "    while True:\n",
    "        jobLst = doc_queue.get()  # Get (doc_id, text) from the queue\n",
    "        passgQ = deque()\n",
    "        if jobLst is None:\n",
    "            break  # Exit if None is received (used for stopping the workers)\n",
    "        else:\n",
    "            tBgn = now()\n",
    "            print( f\"\\nBEGIN processing passages @ {os.getpid()}!\" )\n",
    "\n",
    "        for i, pair_i in enumerate ( jobLst ):\n",
    "            try:\n",
    "                dID_i = pair_i['id' ]\n",
    "                doc_i = pair_i['doc']\n",
    "                vec_i = pair_i['vec']\n",
    "\n",
    "\n",
    "                chunks_i = page_to_sentences( doc_i )\n",
    "                passgs_i = sentences_to_passages( chunks_i, lclEmbd, dID_i, segDiv = _PSG_DIST_DIV ) \n",
    "                \n",
    "                passgQ.extend( passgs_i )  # Safely append result\n",
    "    \n",
    "            except Exception as e:\n",
    "                print( f\"\\nBAD THING @ {os.getpid()}: {e}\\n\" )\n",
    "\n",
    "        m, s  = divmod( now()-tBgn, 60.0  )\n",
    "        Qlen  = len( passgQ )\n",
    "        exTxt = passgQ[0]['txt'] if (Qlen > 0) else ''\n",
    "        print( f\"\\nFINISHED processing {Qlen} passages @ {os.getpid()} after {int(m)}:{s:.2f}!\" )\n",
    "        print( f\"{exTxt}\\n\" )\n",
    "\n",
    "        with lock:\n",
    "            results.extend( list( passgQ ) )\n",
    "    \n",
    "        doc_queue.task_done()  # Signal task completion\n",
    "    \n",
    "\n",
    "def embed_passages_with_process_pool(documents: List[str], chunkSize: int = 10, num_workers: int = 4) -> List[Dict]:\n",
    "    # Initialize manager, queue, and shared resources\n",
    "    manager = multiprocessing.Manager()\n",
    "    doc_queue = multiprocessing.JoinableQueue()\n",
    "    results = manager.list()\n",
    "    # completed_chunks = manager.dict()\n",
    "    lock = manager.Lock()\n",
    "\n",
    "    # Start worker processes\n",
    "    processes = []\n",
    "    for _ in range(num_workers):\n",
    "        p = multiprocessing.Process(\n",
    "            target = passage_process_worker, \n",
    "            args   = (doc_queue, results, lock)\n",
    "        )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    # Enqueue documents with IDs\n",
    "\n",
    "    # Enqueue documents with IDs\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    Npgs = len( documents )\n",
    "    while bgn < Npgs:\n",
    "        end = min( bgn+chunkSize, Npgs )\n",
    "        doc_queue.put( deepcopy( documents[ bgn:end ] ) )\n",
    "        bgn = end\n",
    "    \n",
    "    # Block until all tasks are done\n",
    "    doc_queue.join()\n",
    "\n",
    "    # Stop workers\n",
    "    for _ in range( num_workers ):\n",
    "        doc_queue.put( None )  # Add a None for each worker to signal them to exit\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    return list(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "491b86a9-5e09-4954-ad06-0b58d109de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from utils import RAG_State\n",
    "GRGstate = RAG_State.load_state( environ[\"_RAG_STATE_PATH\"] )\n",
    "\n",
    "def bundle_pages( pageDcts, Ndoc ):\n",
    "    \"\"\" Prepare a subset of pages based on what has already been processed \"\"\"\n",
    "    global GRGstate\n",
    "    rtnBndl = deque()\n",
    "    count   = 0\n",
    "    for i, pair_i in enumerate ( pageDcts ):\n",
    "        dID_i = pair_i['id' ]\n",
    "        if dID_i not in GRGstate.passages['pages']:\n",
    "            rtnBndl.append( pair_i )\n",
    "            count += 1\n",
    "            if count >= Ndoc:\n",
    "                break\n",
    "    return list( rtnBndl )\n",
    "\n",
    "    \n",
    "def generate_passage_chunks( pageDcts, NpageBundle ):\n",
    "    \"\"\" Generate passages for as long as the user allows it \"\"\"\n",
    "    global GRGstate\n",
    "    tBgn = now()\n",
    "    Ntot = 0\n",
    "    Npgs = 0\n",
    "    Nwrk = int( environ[\"_GRG_PASSAGE_N_WORKERS\"] ) \n",
    "    try:\n",
    "        while True:\n",
    "            ## Fetch Pages ##\n",
    "            pgBundle = bundle_pages( pageDcts, NpageBundle )\n",
    "            if len( pgBundle ) == 0:\n",
    "                raise KeyboardInterrupt( \"Ran out of input!\" )\n",
    "            ## Generate Passages ##\n",
    "            clear_output( wait = True )\n",
    "            res = embed_passages_with_process_pool( \n",
    "                pgBundle, \n",
    "                chunkSize   = int(NpageBundle / Nwrk), \n",
    "                num_workers = Nwrk\n",
    "            )\n",
    "            if len( res ) > 0:\n",
    "                Npgs   += len( pgBundle )\n",
    "                Ntot   += len( res )\n",
    "                pklPath = f\"data/Passages_bgn-{pgBundle[0]['id']}.pkl\"\n",
    "                with open( pklPath, 'wb' ) as f:\n",
    "                    pickle.dump( res, f )\n",
    "                M, s = divmod( now()-tBgn, 60.0 )\n",
    "                h, m = divmod( M         , 60.0 )\n",
    "                print( f\"{int(h)}:{int(m)}:{s}, Saved {pklPath}!, {Npgs} --> {Ntot}\" )\n",
    "                GRGstate.passages['pages'].update( [item['id'] for item in pgBundle] )\n",
    "                GRGstate.save_state( environ[\"_RAG_STATE_PATH\"] )\n",
    "    except KeyboardInterrupt:\n",
    "        print( f\"\\nSTOPPED after {(now()-tBgn) / 3600.0} hours!\" )\n",
    "        print( f\"Segmented {Npgs} pages into {Ntot} passages!\" )\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4e9a3-6483-4b1c-a330-1eb1a675989c",
   "metadata": {},
   "source": [
    "Due to mistakes, tuning, and learning, passages from at least 10% of the first 200k pages were extracted poorly:\n",
    "* Passages dropped\n",
    "* Too short\n",
    "* Too long?\n",
    "\n",
    "2024-11-15: Not redoing these at this time, as the system must be robust against fragmentary information.  However, these will need to be redone in the future in order to obtain satisfactory results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58b3ba-61e2-4175-824a-f8a8300f2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGIN processing passages @ 324895!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 324898!\n",
      "BEGIN processing passages @ 324901!\n",
      "\n",
      "BEGIN processing passages @ 324904!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 324907!\n",
      "BEGIN processing passages @ 324910!\n",
      "BEGIN processing passages @ 324916!\n",
      "BEGIN processing passages @ 324913!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 324922!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 324928!\n",
      "\n",
      "BEGIN processing passages @ 324925!\n",
      "BEGIN processing passages @ 324919!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 324931!\n",
      "BEGIN processing passages @ 324937!\n",
      "BEGIN processing passages @ 324934!\n",
      "\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 324940!\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    Nlrg = 160\n",
    "    tBgn = now()\n",
    "    res  = embed_passages_with_process_pool( vecPairs[:Nlrg], chunkSize= 10, num_workers = 16)\n",
    "    m,s  = divmod( now()-tBgn, 60.0 )\n",
    "    print( f\"\\n#####\\nTook {int(m)}:{s:.2f} to process {Nlrg} pages! Segmented {len(res)} passages!\" )\n",
    "else:\n",
    "    generate_passage_chunks( vecPairs, int( environ[\"_GRG_PASSAGE_N_PAGES\"] ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95a734cb-62e9-41be-b552-33a89e3e1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( type( res ), len( res ) )\n",
    "# pprint( res[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f93215-60bf-4a76-b59f-e9b67799151d",
   "metadata": {},
   "source": [
    "## Link Passages to Parent Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5591b7-079f-43b2-abba-0c1f31c89951",
   "metadata": {},
   "source": [
    "## For each passage, Gather pages from N hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "205eb993-7aed-4f25-a61a-4e0a7d861e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in collection.get()['documents']:\n",
    "# # for result in collection.get()['ids']: # 6f83d661-6cdf-46bf-83a4-27f6c36d948f\n",
    "#     # print( result )\n",
    "#     print( dir( result ) )\n",
    "#     break\n",
    "#     # for res in result['ids']:\n",
    "#     #     all_ids.append( res )\n",
    "# # docIDs = list( all_ids )\n",
    "# # print( f\"There are {len(docIDs)} documents!\" )\n",
    "# # print( docIDs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667b5d-ac67-4499-b9e6-c9d8f1b08f28",
   "metadata": {},
   "source": [
    "## Calc ranked passage similarity from those pages, Create up to M connections per passage\n",
    "### (This is a separate collection from page links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4408b94-4953-44fd-aa98-fb31e7bc44af",
   "metadata": {},
   "source": [
    "## Build Passage Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93294997-4586-4cef-918c-b9223d64c839",
   "metadata": {},
   "source": [
    "### Build Passage Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d881b33-1fbe-4c5b-adae-915db03ce34b",
   "metadata": {},
   "source": [
    "### Build Passage Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748be8-aa1b-404b-8efd-b8a7ba1b0a57",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f70d6-f90d-416a-b6d1-7e427f17171b",
   "metadata": {},
   "source": [
    "## What decisions does the retriever have to make?\n",
    "* Following connections\n",
    "* Ranking pages and passages\n",
    "* Stitching passages in proper order.\n",
    "    - What is proper order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dc867-db4b-4f3b-b654-1e735841ad30",
   "metadata": {},
   "source": [
    "## What is our token budget for the LLM summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8de1f-c3d2-4446-8d72-ddc6c61c8289",
   "metadata": {},
   "source": [
    "## What would it look like to extend the budget with overlapping summaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22eaf-d4bf-45b4-bfae-3e54d4d12d97",
   "metadata": {},
   "source": [
    "# Connect to Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "606a31a3-ee34-4a8a-8fe7-0a5669e5a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# def start_arango_container():\n",
    "#     command = [ \"docker\", \"run\", \"-p\", \n",
    "#                 \"8529:8529\", \"-e\", \"ARANGO_ROOT_PASSWORD=\", \"arangodb/arangodb\"]  \n",
    "#     subprocess.Popen( command )\n",
    "\n",
    "# start_arango_container()\n",
    "# sleep( 15.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c1c39f5-d011-4487-8984-9869c3d7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate ArangoDB Database\n",
    "# import json\n",
    "\n",
    "# from adb_cloud_connector import get_temp_credentials\n",
    "# from arango import ArangoClient\n",
    "\n",
    "# con = get_temp_credentials()\n",
    "\n",
    "# db = ArangoClient(hosts=con[\"url\"]).db(\n",
    "#     con[\"dbName\"], con[\"username\"], con[\"password\"], verify=True\n",
    "# )\n",
    "\n",
    "# print(json.dumps(con, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ded7e008-5ddc-4bde-b666-cf6e9e8b9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the ArangoDB-LangChain Graph\n",
    "# from langchain_community.graphs import ArangoGraph\n",
    "\n",
    "# graph = ArangoGraph( db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94314a6a-941b-497b-941a-d95595615e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not db.has_graph( environ[\"_GRG_GRAPH_DB\"] ):\n",
    "#     db.create_graph(\n",
    "#         environ[\"_GRG_GRAPH_DB\"],\n",
    "#         edge_definitions=[\n",
    "#             {\n",
    "#                 \"from_vertex_collections\": [\"subjects\"],\n",
    "#                 \"edge_collection\": \"verbs\",\n",
    "#                 \"to_vertex_collections\": [\"subjects\"],\n",
    "#             },\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a70a3c7-ce94-41b8-9e1c-23a2fdf1761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import docker\n",
    "\n",
    "# client     = docker.from_env()\n",
    "# containers = client.containers.list()\n",
    "\n",
    "# for container in containers:\n",
    "#     print(container.name, container.short_id, container.status)\n",
    "\n",
    "# os.system( \"docker stop 8ed55910d8cc\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799705-7295-4731-b6a8-ee7ba7dacabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
