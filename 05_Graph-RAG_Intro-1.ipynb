{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90a2fda-d974-4e87-8038-f6c5b5bf47f3",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* [AI-Scientist](https://github.com/SakanaAI/AI-Scientist )\n",
    "* [LangChain + Neo4j GRG Tutorial](https://python.langchain.com/docs/tutorials/graph/)\n",
    "* [Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09e7ba-2f31-4bf3-87b9-25cd6df751cf",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f89225-fe56-4b1b-a1da-f8a2be6cdf84",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Notebook 04 Dependencies!\n",
    "* `sudo apt install docker.io`\n",
    "* `sudo chmod 666 /var/run/docker.sock`\n",
    "* `python3.10 -m pip install docker --user`\n",
    "### Install ArangoDB + Docker Container\n",
    "* `sudo docker pull arangodb`\n",
    "* `python3.10 -m pip install python-arango adb-cloud-connector --user`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c74f26-a5d8-4b32-9fa0-83d730041ede",
   "metadata": {},
   "source": [
    "# Inspiration\n",
    "* [Co-STORM @ Stanford](https://storm.genie.stanford.edu/)\n",
    "* [Ellicit](https://elicit.com/)\n",
    "* [Research Rabbit](https://www.researchrabbit.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec290487-1b04-4c74-9f95-a1e413cda7b7",
   "metadata": {},
   "source": [
    "# Init + Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87451cf5-6fc1-4a89-8644-84f31850ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## INIT ####################################################################################\n",
    "import os\n",
    "from os import path, makedirs, environ\n",
    "from utils import copy_pdfs\n",
    "\n",
    "\n",
    "\n",
    "########## ENVIRONMENT #############################################################################\n",
    "\n",
    "##### 04: Basic RAG #######################################################\n",
    "environ[\"_RAG_DOC_DBASE\"]  = \"lit_pdf\"\n",
    "environ[\"_RAG_DOC_EMBED\"]  = \"all-minilm\"\n",
    "\n",
    "##### 05: Graph-RAG (GRG) #################################################\n",
    "environ[\"_GRG_MODEL_NAME\"] = \"llama3.2-vision\"\n",
    "environ[\"_GRG_EMBED_NAME\"] = \"all-minilm\"\n",
    "\n",
    "environ[\"_GRG_GRAPH_DB\"] = \"grg_rel\"\n",
    "\n",
    "##### Flags ###############################################################\n",
    "_LINK_PAGES = True\n",
    "\n",
    "\n",
    "##### Files ###############################################################\n",
    "_PAGE_LINKS = \"data/PageLinksDONE.txt\"\n",
    "_DOC_EMBEDS = \"data/DocVectors.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdea47-2aff-4154-80c7-9fef77992342",
   "metadata": {},
   "source": [
    "### You may need to manually tune these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003c93f9-9c84-461f-b299-790fce004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIM_MAX         =  0.7329191963152777\n",
    "_SIM_MIN         = -0.24439346017677427\n",
    "_MAX_BRANCH      = 10\n",
    "_PAGE_CSN_FRAC   =  0.60 # 1.5% of all possible links\n",
    "_PAGE_CSN_THRESH = (_SIM_MAX-_SIM_MIN) * _PAGE_CSN_FRAC + _SIM_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e6de-09de-41c3-ad97-393d3e3d58ca",
   "metadata": {},
   "source": [
    "# Depth 1: Link PDF Pages by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de0c4d-5605-4d34-a271-c502f63be5f4",
   "metadata": {},
   "source": [
    "## Retrieve 04 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62a0cbf-525b-42d0-9031-e04e8d252f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Loading the vector store sometimes spews warnings\n",
    "import sys\n",
    "\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient();\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b790070a-dbe5-4fed-b01f-8eb06dc69eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vecPairs = None\n",
    "\n",
    "if _LINK_PAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5557def-21c0-40a7-b871-ce4a6fafb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from utils import pull_ollama_model\n",
    "\n",
    "pull_ollama_model( environ[\"_RAG_DOC_EMBED\"] )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c4b50-609c-45bc-a610-5fc5ecbeb226",
   "metadata": {},
   "source": [
    "## Recalculate Embeddings (NOT exposed by ChromaDB!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e42281-259a-4a8f-8432-138cc0fcfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you don’t have enough VRAM it will use the CPU. \n",
    "\n",
    "import time, os\n",
    "now = time.time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "\n",
    "    vecPairs = deque()\n",
    "    docBatch = 1\n",
    "\n",
    "    # Iterate over all documents and collect the IDs\n",
    "    all_ids = deque()\n",
    "    allData = collection.get()\n",
    "    totDocs = allData['documents']\n",
    "    totIDs  = allData['ids']\n",
    "    print( f\"Fetched {len(totDocs)} documents\" )\n",
    "    \n",
    "    tBgn = now()\n",
    "    N    = len( totDocs )\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    # for i, doc in enumerate( totDocs ):\n",
    "    while bgn < N:\n",
    "        end = min( bgn+docBatch, N )\n",
    "        try:\n",
    "            vec = local_embeddings.embed_documents( totDocs[ bgn:end ] )\n",
    "            for i in range( bgn, end ):\n",
    "                vecPairs.append( {'vec' : np.array( vec[i-bgn] ), 'doc': totDocs[i], 'id' : totIDs[i]} )\n",
    "                if ((i+1)%100) == 0:\n",
    "                    print('.',end='',flush=True)\n",
    "                if ((i+1)%10000) == 0:\n",
    "                    m,s = divmod( now()-tBgn, 60 )\n",
    "                    print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "            bgn = end\n",
    "        except Exception as e:\n",
    "            print(e,end=', ',flush=True)\n",
    "            bgn += 1\n",
    "    print( f\"\\nPage embedding recalc took {(now()-tBgn)/60.0:.2f} minutes!\" )\n",
    "\n",
    "    vecPairs = list( vecPairs )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1562139f-e122-4bcf-b428-89da709e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    simMin = 1e9\n",
    "    simMax = 0.0\n",
    "    print( len(vecPairs) )\n",
    "    vectrs = [item['vec'] for item in vecPairs]\n",
    "    vec0   = vectrs[0]\n",
    "    \n",
    "    for vec_i in vectrs[1:]:\n",
    "        sim_i  = cosine_similarity( [vec0, vec_i,] )[0,1]\n",
    "        simMin = min( sim_i, simMin )\n",
    "        simMax = max( sim_i, simMax )\n",
    "    \n",
    "    diffSpan = simMax - simMin\n",
    "    \n",
    "    print( [simMin, simMax,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7afa4e-5b50-46ad-8c81-988a654e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "    with open( _DOC_EMBEDS, 'wb' ) as f:\n",
    "        pickle.dump( vecPairs, f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085508e0-626b-4f3a-b321-ce024b8e8e87",
   "metadata": {},
   "source": [
    "## Calculate Page Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c952741-7fc6-46c8-9e40-76423919e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    \n",
    "    Ndocs     = len( vecPairs )\n",
    "    tBgn      = now()\n",
    "\n",
    "    Nchunk = 500\n",
    "\n",
    "    bgn1 = 0\n",
    "    end1 = 0\n",
    "    bgn2 = 0\n",
    "    end2 = 0\n",
    "\n",
    "    def attempt_connect( pair_i, pair_j ):\n",
    "        sim_ij = cosine_similarity( [pair_i['vec'], pair_j['vec'],] )[0,1]\n",
    "        if sim_ij >= _PAGE_CSN_THRESH:\n",
    "            return {\n",
    "                'type'   : \"Page_Cosine_Similarity\",\n",
    "                'idTail' : pair_i['id'],\n",
    "                'idHead' : pair_j['id'],\n",
    "                'dir'    : False,\n",
    "                'coSim'  : sim_ij,\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    while bgn1 < Ndocs:\n",
    "\n",
    "        pageLinks = deque()\n",
    "\n",
    "        ## Define Chunks 1 & 2 ##\n",
    "        end1 = min( bgn1+Nchunk, Ndocs )\n",
    "        bgn2 = end1 if (end1 < Ndocs) else 0 # Chunk 2 wraps at the end\n",
    "        end2 = min( bgn2+Nchunk, Ndocs )\n",
    "\n",
    "        chnk1 = vecPairs[ bgn1:end1 ]\n",
    "        chnk2 = vecPairs[ bgn2:end2 ]\n",
    "\n",
    "        N1 = end1 - bgn1\n",
    "        N2 = end2 - bgn2\n",
    "\n",
    "        ## Connections between Chunks 1 & 2 ##\n",
    "        for i in range( N1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('.',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( N2 ):\n",
    "                pair_j = chnk2[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        ## Connections within Chunk 1 ##\n",
    "        for i in range( N1-1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('~',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( i+1, N1 ):\n",
    "                pair_j = chnk1[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        pageLinks = list( pageLinks )\n",
    "        with open( f\"data/PageLinks_{bgn1}-{end2}.pkl\", 'wb' ) as f:\n",
    "            pickle.dump( pageLinks, f )\n",
    "        \n",
    "        m,s = divmod( now()-tBgn, 60 )\n",
    "        print(f\"\\n{bgn1}:{end1}/{bgn2}:{end2}, {int(m)}:{s:.2f}, {len(pageLinks)},\",end=' ',flush=True)\n",
    "        bgn1 = end1\n",
    "\n",
    "    os.system( f\"touch {_PAGE_LINKS}\" )\n",
    "    print( f\"\\nBuilt page graph in {(now()-tBgn)/60.0/60.0:.2f} hours!\" )\n",
    "    print()\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133da01-c15b-4427-ac1b-815510e2960b",
   "metadata": {},
   "source": [
    "## Build Page Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724cfa6-6819-4f76-a9fd-5efd1d813438",
   "metadata": {},
   "source": [
    "### Build Page Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ba23d-af9c-40cf-83e5-643b79671f6a",
   "metadata": {},
   "source": [
    "### Build Page Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5117a2e-6a1d-4f3c-9258-6a5da33c31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f\"There are {len(pageLinks)} connections between pages!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0337e-e003-45c0-9a5d-ca042a5191cc",
   "metadata": {},
   "source": [
    "# Depth 2: Link Passages by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e83c19-60b7-4fb1-ba5b-5d737ad0d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "_GEN_PASSAGES = True\n",
    "_PASSAGE_FNAM = \"data/Passages.pkl\"\n",
    "_PSG_DIST_DIV = 1.41 # Lower number, Few Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f569ef3-fecf-4c82-be8c-181546e32fe7",
   "metadata": {},
   "source": [
    "## Segment Pages Into Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4949e3d5-163d-4366-848a-fbb2d82b1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def page_to_sentences( pageText ):\n",
    "    \"\"\" Parse the page into individual sentences \"\"\"\n",
    "    rtnParts = deque()\n",
    "    pageText = fr\"{pageText}\" + '.' # Terminator hack\n",
    "    sepChars = ['?','!','\\n']\n",
    "    sepPhras = ['. ',]\n",
    "    sentence = \"\"\n",
    "    word     = \"\"\n",
    "\n",
    "    # print( len(pageText) )\n",
    "    # pprint( pageText )\n",
    "\n",
    "    def push_chunk():\n",
    "        nonlocal rtnParts, sentence, word\n",
    "        rtnParts.append( sentence )\n",
    "        sentence = \"\"\n",
    "        word     = \"\"\n",
    "    \n",
    "    for c in pageText:\n",
    "        sentence += c # Include punctuation in the sentence\n",
    "        word     += c\n",
    "        if (c in sepChars) or (word in sepPhras):\n",
    "            push_chunk()\n",
    "\n",
    "    chunks = list( rtnParts )\n",
    "    chunks = [str( part ).strip() for part in chunks]\n",
    "\n",
    "    # print( chunks )\n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9958dc-c23b-4003-b0e4-2108294715ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_ID\n",
    "\n",
    "def sentences_to_passages( chunkList, embedder, parentID, segDiv = 10.0 ):\n",
    "    \"\"\" Segment a list of sentences into a passage \"\"\"\n",
    "    vectors = embedder.embed_documents( chunkList )\n",
    "    senVecs = zip( vectors, chunkList )\n",
    "    rtnSeg  = deque()\n",
    "\n",
    "    def vec_diff( v1, v2 ):\n",
    "        \"\"\" Distance between 2 pnts \"\"\"\n",
    "        return np.linalg.norm( np.subtract( v1, v2 ) )\n",
    "\n",
    "    def get_total_width():\n",
    "        nonlocal vectors\n",
    "        N    = len( vectors )\n",
    "        dMax = -1.0\n",
    "        for i in range( N-1 ):\n",
    "            for j in range( i+1, N ):\n",
    "                dMax = max( dMax, vec_diff( vectors[i], vectors[j] ) )\n",
    "        return dMax\n",
    "\n",
    "    segRad = get_total_width() / segDiv\n",
    "    \n",
    "    vecs_j = list()\n",
    "    mean_j = None\n",
    "    txtP_j = \"\"\n",
    "\n",
    "    for i, (vec_i, sen_i) in enumerate( senVecs ):\n",
    "\n",
    "        if i == 0:\n",
    "            mean_j = vec_i\n",
    "\n",
    "        if vec_diff( vec_i, mean_j ) > segRad:\n",
    "            rtnSeg.append({\n",
    "                'id'    : gen_ID(),\n",
    "                'vec'   : embedder.embed_documents( [txtP_j,] )[0], # Embedding might be different than the average?\n",
    "                'txt'   : txtP_j,\n",
    "                'pageID': str( parentID ),\n",
    "            })\n",
    "            vecs_j = [vec_i,]\n",
    "            mean_j = vec_i\n",
    "            txtP_j = sen_i\n",
    "        else:\n",
    "            vecs_j.append( vec_i )\n",
    "            mean_j = np.mean( vecs_j, axis = 0 )\n",
    "            txtP_j += ' ' + sen_i # Reinsert leading space\n",
    "\n",
    "    return list( rtnSeg )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032de2a1-522d-4884-a44f-ca353de25612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "vecPairs = list()\n",
    "if _GEN_PASSAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d44168e-34d6-4043-a6c1-60caf1f06930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........~......"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m vec_i \u001b[38;5;241m=\u001b[39m pair_i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvec\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m chunks_i \u001b[38;5;241m=\u001b[39m page_to_sentences( doc_i )\n\u001b[0;32m---> 18\u001b[0m passgs_i \u001b[38;5;241m=\u001b[39m \u001b[43msentences_to_passages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdID_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegDiv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.41\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Lower number, Few Segments\u001b[39;00m\n\u001b[1;32m     19\u001b[0m passages\u001b[38;5;241m.\u001b[39mextend( passgs_i )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36msentences_to_passages\u001b[0;34m(chunkList, embedder, parentID, segDiv)\u001b[0m\n\u001b[1;32m     31\u001b[0m     mean_j \u001b[38;5;241m=\u001b[39m vec_i\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vec_diff( vec_i, mean_j ) \u001b[38;5;241m>\u001b[39m segRad:\n\u001b[1;32m     34\u001b[0m     rtnSeg\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m    : gen_ID(),\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvec\u001b[39m\u001b[38;5;124m'\u001b[39m   : \u001b[43membedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtxtP_j\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;66;03m# Embedding might be different than the average?\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m   : txtP_j,\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpageID\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m( parentID ),\n\u001b[1;32m     39\u001b[0m     })\n\u001b[1;32m     40\u001b[0m     vecs_j \u001b[38;5;241m=\u001b[39m [vec_i,]\n\u001b[1;32m     41\u001b[0m     mean_j \u001b[38;5;241m=\u001b[39m vec_i\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_ollama/embeddings.py:159\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Embed search docs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     embedded_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embedded_docs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ollama/_client.py:262\u001b[0m, in \u001b[0;36mClient.embed\u001b[0;34m(self, model, input, truncate, options, keep_alive)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model:\n\u001b[1;32m    260\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m RequestError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust provide a model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/embed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtruncate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ollama/_client.py:70\u001b[0m, in \u001b[0;36mClient._request\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m---> 70\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:837\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    822\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m    824\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    825\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    826\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    836\u001b[0m )\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# NOTE: THIS IS A 30 DAY CALCULATION!!! 3.6 HOURS PER 10k PAGES! RUN JOBS AS AVAILABLE!\n",
    "\n",
    "\n",
    "if _GEN_PASSAGES and (not os.path.isfile( _PASSAGE_FNAM )):\n",
    "\n",
    "    passages = deque()\n",
    "    tBgn     = now()\n",
    "\n",
    "    # FIXME: PICK UP FROM LAST 10k INDEX @ RAGSTATE\n",
    "    for i, pair_i in enumerate ( vecPairs ):\n",
    "\n",
    "        try:\n",
    "        \n",
    "            dID_i = pair_i['id' ]\n",
    "            doc_i = pair_i['doc']\n",
    "            vec_i = pair_i['vec']\n",
    "        \n",
    "            chunks_i = page_to_sentences( doc_i )\n",
    "            passgs_i = sentences_to_passages( chunks_i, local_embeddings, dID_i, segDiv = _PSG_DIST_DIV ) \n",
    "            passages.extend( passgs_i )\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "\n",
    "        if ((i+1)%10) == 0:\n",
    "            print('.',end='',flush=True)\n",
    "        if ((i+1)%100) == 0:\n",
    "            print('~',end='',flush=True)\n",
    "            \n",
    "        if ((i+1)%10000) == 0:\n",
    "            # FIXME: SEND LAST 10k INDEX TO RAGSTATE\n",
    "            # FIXME: PICKLE EACH 10k BATCH\n",
    "            # FIXME: RESET `passages` HERE\n",
    "            m,s = divmod( now()-tBgn, 60 )\n",
    "            print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "\n",
    "    passages = list( passages )\n",
    "\n",
    "    with open( _PASSAGE_FNAM, 'wb' ) as f:\n",
    "        pickle.dump( passages, f )\n",
    "    \n",
    "    print( f\"\\nCalculated passages in {(now()-tBgn)/60.0/60.0:.2f} hours!\" )\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # print( len( passgs_i ) )\n",
    "    # for psg in passgs_i:\n",
    "    #     print( f\"{psg['txt']}\\n\" )\n",
    "    \n",
    "    # # pprint( passgs_i[4] )\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793761a-46ae-48f6-9466-c695a0a30da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528eb7f-282b-4fc8-80c7-119437d0a6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a52a757-4438-4d51-bbb9-e512221330c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5f93215-60bf-4a76-b59f-e9b67799151d",
   "metadata": {},
   "source": [
    "## Link Passages to Parent Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5591b7-079f-43b2-abba-0c1f31c89951",
   "metadata": {},
   "source": [
    "## For each passage, Gather pages from N hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eb993-7aed-4f25-a61a-4e0a7d861e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in collection.get()['documents']:\n",
    "# # for result in collection.get()['ids']: # 6f83d661-6cdf-46bf-83a4-27f6c36d948f\n",
    "#     # print( result )\n",
    "#     print( dir( result ) )\n",
    "#     break\n",
    "#     # for res in result['ids']:\n",
    "#     #     all_ids.append( res )\n",
    "# # docIDs = list( all_ids )\n",
    "# # print( f\"There are {len(docIDs)} documents!\" )\n",
    "# # print( docIDs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667b5d-ac67-4499-b9e6-c9d8f1b08f28",
   "metadata": {},
   "source": [
    "## Calc ranked passage similarity from those pages, Create up to M connections per passage\n",
    "### (This is a separate collection from page links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4408b94-4953-44fd-aa98-fb31e7bc44af",
   "metadata": {},
   "source": [
    "## Build Passage Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93294997-4586-4cef-918c-b9223d64c839",
   "metadata": {},
   "source": [
    "### Build Passage Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d881b33-1fbe-4c5b-adae-915db03ce34b",
   "metadata": {},
   "source": [
    "### Build Passage Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748be8-aa1b-404b-8efd-b8a7ba1b0a57",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f70d6-f90d-416a-b6d1-7e427f17171b",
   "metadata": {},
   "source": [
    "## What decisions does the retriever have to make?\n",
    "* Following connections\n",
    "* Ranking pages and passages\n",
    "* Stitching passages in proper order.\n",
    "    - What is proper order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dc867-db4b-4f3b-b654-1e735841ad30",
   "metadata": {},
   "source": [
    "## What is our token budget for the LLM summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8de1f-c3d2-4446-8d72-ddc6c61c8289",
   "metadata": {},
   "source": [
    "## What would it look like to extend the budget with overlapping summaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d349b-dd4d-4935-9e76-54256f8720de",
   "metadata": {},
   "source": [
    "# Create Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5804d7-6971-41b3-b303-78c3b21ab5da",
   "metadata": {},
   "source": [
    "## What would it look like for the summarizer to extend the token budget with Chain of Thought \"Reasoning\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e304759-e7fc-4735-ba76-36ed434e666c",
   "metadata": {},
   "source": [
    "# Depth 3: Build Statements (Assumptions and Claims) about Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905b2d8-6ba3-41c3-b1fd-a2f6b14c35a4",
   "metadata": {},
   "source": [
    "## Segment Keywords\n",
    "* Statistically important/rare phrases\n",
    "* Ask LLM to isolate {jargon, technical terms}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c77203-fff8-4db0-b1a3-5298ee1bf3ed",
   "metadata": {},
   "source": [
    "## Build Statements (Assumptions and Claims)\n",
    "* S-V-O sentence-by-sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd1625-3ff4-4a0e-bbb6-a3755650b554",
   "metadata": {},
   "source": [
    "# Depth 4: Support/Refute Statements (Assumptions and Claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193e0d7-3037-4eba-ab08-e2b574f4256d",
   "metadata": {},
   "source": [
    "## How to determine support?  How to determine contradiction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc56c5-df26-4305-997d-25950bcfaf88",
   "metadata": {},
   "source": [
    "## How to weigh support/contradiction based on our level of trust in existing documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe82a-21c1-410e-8757-11a0b5638820",
   "metadata": {},
   "source": [
    "# Advanced Knowledge Graph Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaf6d7-3549-40da-b28a-084eaf4b630b",
   "metadata": {},
   "source": [
    "## Statements: What is the difference between an Assumption and a Claim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6ca8f-a0b6-4abf-83a7-e81e5e22bbc2",
   "metadata": {},
   "source": [
    "## What assumptions do we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43bb88-9a67-4b51-8aac-a05aa43c2a43",
   "metadata": {},
   "source": [
    "## What claims are supported by assumptions we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f335b0-9f35-4cfd-b5ad-1c64252c7e91",
   "metadata": {},
   "source": [
    "## What questions are being asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b235e-2b90-4297-9ce0-c941a89c58c1",
   "metadata": {},
   "source": [
    "## What questions are being answered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0552f-3c89-4de4-9da9-a087489a16b2",
   "metadata": {},
   "source": [
    "## Does the document create new trustworthy connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3409c-8f9c-4a75-983f-5d4ab6c1a37b",
   "metadata": {},
   "source": [
    "## Can we find a similar subgraph in a different field of research?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286e3d5-056e-44af-bf9e-2d06941da2c9",
   "metadata": {},
   "source": [
    "## What questions CAN be asked based on the movement of passages and concepts through vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a9556-3072-4def-9292-31ff04f064b9",
   "metadata": {},
   "source": [
    "## Can we track trajectories in vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b0a23-d4c6-4aa6-8e44-8957200a58d5",
   "metadata": {},
   "source": [
    "## Can we PREDICT trajectories in vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e4882-5e86-4a22-905e-e4917c82e413",
   "metadata": {},
   "source": [
    "# KNNEST: Knowledge Graph Structure Notes\n",
    "* How to know sources are in different fields? ~ Cluster embeddings?\n",
    "* New heirarchical embedding per field?\n",
    "* Do embeddings need to be compressed by PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823f6b4-3876-4c77-995b-9de8c4419ecd",
   "metadata": {},
   "source": [
    "# ANTs: Search Agent Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acd01b-9797-4d5a-a9b4-f80985a4076e",
   "metadata": {},
   "source": [
    "## Can we TRAIN an agent to make advantageous traversals on the KG based on vector space deltas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d1b4b-db2e-43d7-bcf0-074a14e67167",
   "metadata": {},
   "source": [
    "## Swarm-Level Load Management\n",
    "* Agent Instantiation Condition(s)\n",
    "* Agent Deletion Condition(s)\n",
    "* Task Start Condition(s)\n",
    "* Task Stop Condition(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71fee0-1a30-4438-8049-75b49e2f0b75",
   "metadata": {},
   "source": [
    "## ANT Decision-Making Architecture\n",
    "* Resource alotment: {Time, Compute}\n",
    "* Critical: What edge to follow?\n",
    "* A strong line of reasoning can be a demonstration trajectory? LfD?\n",
    "* Inverse-RL to produce an evaluation function for trajectories thru the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22eaf-d4bf-45b4-bfae-3e54d4d12d97",
   "metadata": {},
   "source": [
    "# Connect to Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a31a3-ee34-4a8a-8fe7-0a5669e5a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# def start_arango_container():\n",
    "#     command = [ \"docker\", \"run\", \"-p\", \n",
    "#                 \"8529:8529\", \"-e\", \"ARANGO_ROOT_PASSWORD=\", \"arangodb/arangodb\"]  \n",
    "#     subprocess.Popen( command )\n",
    "\n",
    "# start_arango_container()\n",
    "# sleep( 15.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c39f5-d011-4487-8984-9869c3d7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate ArangoDB Database\n",
    "# import json\n",
    "\n",
    "# from adb_cloud_connector import get_temp_credentials\n",
    "# from arango import ArangoClient\n",
    "\n",
    "# con = get_temp_credentials()\n",
    "\n",
    "# db = ArangoClient(hosts=con[\"url\"]).db(\n",
    "#     con[\"dbName\"], con[\"username\"], con[\"password\"], verify=True\n",
    "# )\n",
    "\n",
    "# print(json.dumps(con, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7e008-5ddc-4bde-b666-cf6e9e8b9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the ArangoDB-LangChain Graph\n",
    "# from langchain_community.graphs import ArangoGraph\n",
    "\n",
    "# graph = ArangoGraph( db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94314a6a-941b-497b-941a-d95595615e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not db.has_graph( environ[\"_GRG_GRAPH_DB\"] ):\n",
    "#     db.create_graph(\n",
    "#         environ[\"_GRG_GRAPH_DB\"],\n",
    "#         edge_definitions=[\n",
    "#             {\n",
    "#                 \"from_vertex_collections\": [\"subjects\"],\n",
    "#                 \"edge_collection\": \"verbs\",\n",
    "#                 \"to_vertex_collections\": [\"subjects\"],\n",
    "#             },\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70a3c7-ce94-41b8-9e1c-23a2fdf1761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import docker\n",
    "\n",
    "# client     = docker.from_env()\n",
    "# containers = client.containers.list()\n",
    "\n",
    "# for container in containers:\n",
    "#     print(container.name, container.short_id, container.status)\n",
    "\n",
    "# os.system( \"docker stop 8ed55910d8cc\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39eb5c6-6c53-4c30-a1e3-80ce01551f6c",
   "metadata": {},
   "source": [
    "# Wouldn't it be cool if ...\n",
    "* A manifold (high-order hypersurface) could be fit to the motion of human knowledge\n",
    "* We could trace paths on that manifold\n",
    "* Given a position and curvature (+jerk+snap+crackle+pop), we can extrapolate motion beyond the edge of the mapped manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799705-7295-4731-b6a8-ee7ba7dacabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
