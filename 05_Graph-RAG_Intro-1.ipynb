{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90a2fda-d974-4e87-8038-f6c5b5bf47f3",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* [AI-Scientist](https://github.com/SakanaAI/AI-Scientist )\n",
    "* [LangChain + Neo4j GRG Tutorial](https://python.langchain.com/docs/tutorials/graph/)\n",
    "* [Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09e7ba-2f31-4bf3-87b9-25cd6df751cf",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f89225-fe56-4b1b-a1da-f8a2be6cdf84",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Notebook 04 Dependencies!\n",
    "* `sudo apt install docker.io`\n",
    "* `sudo chmod 666 /var/run/docker.sock`\n",
    "* `python3.10 -m pip install docker --user`\n",
    "### Install ArangoDB + Docker Container\n",
    "* `sudo docker pull arangodb`\n",
    "* `python3.10 -m pip install python-arango adb-cloud-connector --user`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c74f26-a5d8-4b32-9fa0-83d730041ede",
   "metadata": {},
   "source": [
    "# Inspiration\n",
    "* [Co-STORM @ Stanford](https://storm.genie.stanford.edu/)\n",
    "* [Ellicit](https://elicit.com/)\n",
    "* [Research Rabbit](https://www.researchrabbit.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec290487-1b04-4c74-9f95-a1e413cda7b7",
   "metadata": {},
   "source": [
    "# Init + Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87451cf5-6fc1-4a89-8644-84f31850ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## INIT ####################################################################################\n",
    "import os\n",
    "from os import path, makedirs, environ\n",
    "from utils import copy_pdfs\n",
    "\n",
    "\n",
    "\n",
    "########## ENVIRONMENT #############################################################################\n",
    "\n",
    "##### 04: Basic RAG #######################################################\n",
    "environ[\"_RAG_DOC_DBASE\" ] = \"lit_pdf\"\n",
    "environ[\"_RAG_DOC_EMBED\" ] = \"all-minilm\"\n",
    "environ[\"_RAG_STATE_PATH\"] = \"data/state.json\"\n",
    "\n",
    "##### 05: Graph-RAG (GRG) #################################################\n",
    "environ[\"_GRG_MODEL_NAME\"] = \"llama3.2-vision\"\n",
    "environ[\"_GRG_EMBED_NAME\"] = \"all-minilm\"\n",
    "\n",
    "environ[\"_GRG_GRAPH_DB\"] = \"grg_rel\"\n",
    "\n",
    "##### Flags ###############################################################\n",
    "_LINK_PAGES = True\n",
    "\n",
    "\n",
    "##### Files ###############################################################\n",
    "_PAGE_LINKS = \"data/PageLinksDONE.txt\"\n",
    "_DOC_EMBEDS = \"data/DocVectors.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdea47-2aff-4154-80c7-9fef77992342",
   "metadata": {},
   "source": [
    "### You may need to manually tune these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "003c93f9-9c84-461f-b299-790fce004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIM_MAX         =  0.7329191963152777\n",
    "_SIM_MIN         = -0.24439346017677427\n",
    "_MAX_BRANCH      = 10\n",
    "_PAGE_CSN_FRAC   =  0.60 # 1.5% of all possible links\n",
    "_PAGE_CSN_THRESH = (_SIM_MAX-_SIM_MIN) * _PAGE_CSN_FRAC + _SIM_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e6de-09de-41c3-ad97-393d3e3d58ca",
   "metadata": {},
   "source": [
    "# Depth 1: Link PDF Pages by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de0c4d-5605-4d34-a271-c502f63be5f4",
   "metadata": {},
   "source": [
    "## Retrieve 04 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62a0cbf-525b-42d0-9031-e04e8d252f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Loading the vector store sometimes spews warnings\n",
    "import sys\n",
    "\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient();\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b790070a-dbe5-4fed-b01f-8eb06dc69eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vecPairs = None\n",
    "\n",
    "if _LINK_PAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5557def-21c0-40a7-b871-ce4a6fafb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from utils import pull_ollama_model\n",
    "\n",
    "pull_ollama_model( environ[\"_RAG_DOC_EMBED\"] )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c4b50-609c-45bc-a610-5fc5ecbeb226",
   "metadata": {},
   "source": [
    "## Recalculate Embeddings (NOT exposed by ChromaDB!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e42281-259a-4a8f-8432-138cc0fcfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you don’t have enough VRAM it will use the CPU. \n",
    "\n",
    "import time, os\n",
    "now = time.time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "\n",
    "    vecPairs = deque()\n",
    "    docBatch = 1\n",
    "\n",
    "    # Iterate over all documents and collect the IDs\n",
    "    all_ids = deque()\n",
    "    allData = collection.get()\n",
    "    totDocs = allData['documents']\n",
    "    totIDs  = allData['ids']\n",
    "    print( f\"Fetched {len(totDocs)} documents\" )\n",
    "    \n",
    "    tBgn = now()\n",
    "    N    = len( totDocs )\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    # for i, doc in enumerate( totDocs ):\n",
    "    while bgn < N:\n",
    "        end = min( bgn+docBatch, N )\n",
    "        try:\n",
    "            vec = local_embeddings.embed_documents( totDocs[ bgn:end ] )\n",
    "            for i in range( bgn, end ):\n",
    "                vecPairs.append( {'vec' : np.array( vec[i-bgn] ), 'doc': totDocs[i], 'id' : totIDs[i]} )\n",
    "                if ((i+1)%100) == 0:\n",
    "                    print('.',end='',flush=True)\n",
    "                if ((i+1)%10000) == 0:\n",
    "                    m,s = divmod( now()-tBgn, 60 )\n",
    "                    print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "            bgn = end\n",
    "        except Exception as e:\n",
    "            print(e,end=', ',flush=True)\n",
    "            bgn += 1\n",
    "    print( f\"\\nPage embedding recalc took {(now()-tBgn)/60.0:.2f} minutes!\" )\n",
    "\n",
    "    vecPairs = list( vecPairs )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1562139f-e122-4bcf-b428-89da709e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    simMin = 1e9\n",
    "    simMax = 0.0\n",
    "    print( len(vecPairs) )\n",
    "    vectrs = [item['vec'] for item in vecPairs]\n",
    "    vec0   = vectrs[0]\n",
    "    \n",
    "    for vec_i in vectrs[1:]:\n",
    "        sim_i  = cosine_similarity( [vec0, vec_i,] )[0,1]\n",
    "        simMin = min( sim_i, simMin )\n",
    "        simMax = max( sim_i, simMax )\n",
    "    \n",
    "    diffSpan = simMax - simMin\n",
    "    \n",
    "    print( [simMin, simMax,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec7afa4e-5b50-46ad-8c81-988a654e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "    with open( _DOC_EMBEDS, 'wb' ) as f:\n",
    "        pickle.dump( vecPairs, f )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085508e0-626b-4f3a-b321-ce024b8e8e87",
   "metadata": {},
   "source": [
    "## Calculate Page Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c952741-7fc6-46c8-9e40-76423919e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    \n",
    "    Ndocs     = len( vecPairs )\n",
    "    tBgn      = now()\n",
    "\n",
    "    Nchunk = 500\n",
    "\n",
    "    bgn1 = 0\n",
    "    end1 = 0\n",
    "    bgn2 = 0\n",
    "    end2 = 0\n",
    "\n",
    "    def attempt_connect( pair_i, pair_j ):\n",
    "        sim_ij = cosine_similarity( [pair_i['vec'], pair_j['vec'],] )[0,1]\n",
    "        if sim_ij >= _PAGE_CSN_THRESH:\n",
    "            return {\n",
    "                'type'   : \"Page_Cosine_Similarity\",\n",
    "                'idTail' : pair_i['id'],\n",
    "                'idHead' : pair_j['id'],\n",
    "                'dir'    : False,\n",
    "                'coSim'  : sim_ij,\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    while bgn1 < Ndocs:\n",
    "\n",
    "        pageLinks = deque()\n",
    "\n",
    "        ## Define Chunks 1 & 2 ##\n",
    "        end1 = min( bgn1+Nchunk, Ndocs )\n",
    "        bgn2 = end1 if (end1 < Ndocs) else 0 # Chunk 2 wraps at the end\n",
    "        end2 = min( bgn2+Nchunk, Ndocs )\n",
    "\n",
    "        chnk1 = vecPairs[ bgn1:end1 ]\n",
    "        chnk2 = vecPairs[ bgn2:end2 ]\n",
    "\n",
    "        N1 = end1 - bgn1\n",
    "        N2 = end2 - bgn2\n",
    "\n",
    "        ## Connections between Chunks 1 & 2 ##\n",
    "        for i in range( N1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('.',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( N2 ):\n",
    "                pair_j = chnk2[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        ## Connections within Chunk 1 ##\n",
    "        for i in range( N1-1 ):\n",
    "            if ((i+1)%10==0):\n",
    "                print('~',end='',flush=True)\n",
    "            pair_i = chnk1[i]\n",
    "            Nconn  = 0\n",
    "            for j in range( i+1, N1 ):\n",
    "                pair_j = chnk1[j]\n",
    "                res_ij = attempt_connect( pair_i, pair_j )\n",
    "                if res_ij is not None:\n",
    "                    pageLinks.append( res_ij )\n",
    "                    Nconn += 1\n",
    "                    if Nconn >= _MAX_BRANCH:\n",
    "                        break\n",
    "\n",
    "        pageLinks = list( pageLinks )\n",
    "        with open( f\"data/PageLinks_{bgn1}-{end2}.pkl\", 'wb' ) as f:\n",
    "            pickle.dump( pageLinks, f )\n",
    "        \n",
    "        m,s = divmod( now()-tBgn, 60 )\n",
    "        print(f\"\\n{bgn1}:{end1}/{bgn2}:{end2}, {int(m)}:{s:.2f}, {len(pageLinks)},\",end=' ',flush=True)\n",
    "        bgn1 = end1\n",
    "\n",
    "    os.system( f\"touch {_PAGE_LINKS}\" )\n",
    "    print( f\"\\nBuilt page graph in {(now()-tBgn)/60.0/60.0:.2f} hours!\" )\n",
    "    print()\n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133da01-c15b-4427-ac1b-815510e2960b",
   "metadata": {},
   "source": [
    "## Build Page Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724cfa6-6819-4f76-a9fd-5efd1d813438",
   "metadata": {},
   "source": [
    "### Build Page Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542ba23d-af9c-40cf-83e5-643b79671f6a",
   "metadata": {},
   "source": [
    "### Build Page Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5117a2e-6a1d-4f3c-9258-6a5da33c31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( f\"There are {len(pageLinks)} connections between pages!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0337e-e003-45c0-9a5d-ca042a5191cc",
   "metadata": {},
   "source": [
    "# Depth 2: Link Passages by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e83c19-60b7-4fb1-ba5b-5d737ad0d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "_GEN_PASSAGES = True\n",
    "_PASSAGE_FNAM = \"data/Passages.pkl\"\n",
    "_PSG_DIST_DIV = 1.41 # Lower number, Few Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20fe63ce-347b-410c-a036-a9eedb101d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "environ[\"OLLAMA_NUM_PARALLEL\"]      =  \"16\"\n",
    "environ[\"OLLAMA_MAX_LOADED_MODELS\"] =   \"4\"\n",
    "environ[\"_GRG_PASSAGE_N_WORKERS\"]   =  \"16\"\n",
    "environ[\"_GRG_PASSAGE_N_PAGES\"]     = \"160\" # About 20 mins for 16 workers, Low for TESTING\n",
    "# environ[\"_GRG_PASSAGE_N_PAGES\"]     = \"640\" # About 80 mins for 16 workers?, 300+ PKL files\n",
    "\n",
    "\"\"\"\n",
    "PAR/MO/WR/PGS:         | 200k pgs (Round-the-Clock)\n",
    " 32/ 8/ 4/ 80: ~12 min / 20 days\n",
    " 64/16/ 8/ 80: ~11 min / 19 days\n",
    " 32/ 8/ 8/ 80: ~11 min / 19 days\n",
    " 16/ 4/ 8/ 80: ~11 min / 19 days\n",
    "128/32/ 8/ 80: ~11 min / 19 days\n",
    " 16/ 4/16/160: ~20 min / 17 days *** Winner\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f569ef3-fecf-4c82-be8c-181546e32fe7",
   "metadata": {},
   "source": [
    "## Segment Pages Into Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4949e3d5-163d-4366-848a-fbb2d82b1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def page_to_sentences( pageText ):\n",
    "    \"\"\" Parse the page into individual sentences \"\"\"\n",
    "    rtnParts = deque()\n",
    "    pageText = fr\"{pageText}\" + '.' # Terminator hack\n",
    "    sepChars = ['?','!','\\n']\n",
    "    sepPhras = ['. ',]\n",
    "    sentence = \"\"\n",
    "    word     = \"\"\n",
    "\n",
    "    # print( len(pageText) )\n",
    "    # pprint( pageText )\n",
    "\n",
    "    def push_chunk():\n",
    "        nonlocal rtnParts, sentence, word\n",
    "        rtnParts.append( sentence )\n",
    "        sentence = \"\"\n",
    "        word     = \"\"\n",
    "    \n",
    "    for c in pageText:\n",
    "        sentence += c # Include punctuation in the sentence\n",
    "        word     += c\n",
    "        if (c in sepChars) or (word in sepPhras):\n",
    "            push_chunk()\n",
    "\n",
    "    chunks = list( rtnParts )\n",
    "    chunks = [str( part ).strip() for part in chunks]\n",
    "\n",
    "    # print( chunks )\n",
    "\n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d9958dc-c23b-4003-b0e4-2108294715ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gen_ID\n",
    "\n",
    "def sentences_to_passages( chunkList, embedder, parentID, segDiv = 10.0 ):\n",
    "    \"\"\" Segment a list of sentences into a passage \"\"\"\n",
    "    vectors = embedder.embed_documents( chunkList )\n",
    "    senVecs = zip( vectors, chunkList )\n",
    "    rtnSeg  = deque()\n",
    "\n",
    "    def vec_diff( v1, v2 ):\n",
    "        \"\"\" Distance between 2 pnts \"\"\"\n",
    "        return np.linalg.norm( np.subtract( v1, v2 ) )\n",
    "\n",
    "    def get_total_width():\n",
    "        nonlocal vectors\n",
    "        N    = len( vectors )\n",
    "        dMax = -1.0\n",
    "        for i in range( N-1 ):\n",
    "            for j in range( i+1, N ):\n",
    "                dMax = max( dMax, vec_diff( vectors[i], vectors[j] ) )\n",
    "        return dMax\n",
    "\n",
    "    segRad = get_total_width() / segDiv\n",
    "    \n",
    "    vecs_j = list()\n",
    "    mean_j = None\n",
    "    txtP_j = \"\"\n",
    "\n",
    "    for i, (vec_i, sen_i) in enumerate( senVecs ):\n",
    "\n",
    "        if i == 0:\n",
    "            mean_j = vec_i\n",
    "\n",
    "        if vec_diff( vec_i, mean_j ) > segRad:\n",
    "            rtnSeg.append({\n",
    "                'id'    : gen_ID(),\n",
    "                'vec'   : embedder.embed_documents( [txtP_j,] )[0], # Embedding might be different than the average?\n",
    "                'txt'   : txtP_j,\n",
    "                'pageID': str( parentID ),\n",
    "            })\n",
    "            vecs_j = [vec_i,]\n",
    "            mean_j = vec_i\n",
    "            txtP_j = sen_i\n",
    "        else:\n",
    "            vecs_j.append( vec_i )\n",
    "            mean_j = np.mean( vecs_j, axis = 0 )\n",
    "            txtP_j += ' ' + sen_i # Reinsert leading space\n",
    "\n",
    "    return list( rtnSeg )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "032de2a1-522d-4884-a44f-ca353de25612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "vecPairs = list()\n",
    "if _GEN_PASSAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "295ad5ee-f325-459d-98a3-1ab4ad76b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from typing import List, Dict\n",
    "from copy import deepcopy\n",
    "\n",
    "# # Initialize the embedding model globally, as each process will need access to it\n",
    "# embedding_model = OllamaEmbeddings(model_name=\"all-MiniLM\")  # Configure model if needed\n",
    "\n",
    "# Define the worker function\n",
    "def passage_process_worker(doc_queue: multiprocessing.Queue, results: List[Dict], lock: multiprocessing.Lock):\n",
    "    \n",
    "    # print( f\"\\nBegin processing passages @ {os.getpid()}!\" )\n",
    "    lclEmbd  = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )\n",
    "    tBgn     = 0.0 \n",
    "\n",
    "    while True:\n",
    "        jobLst = doc_queue.get()  # Get (doc_id, text) from the queue\n",
    "        passgQ = deque()\n",
    "        if jobLst is None:\n",
    "            break  # Exit if None is received (used for stopping the workers)\n",
    "        else:\n",
    "            tBgn = now()\n",
    "            print( f\"\\nBEGIN processing passages @ {os.getpid()}!\" )\n",
    "\n",
    "        for i, pair_i in enumerate ( jobLst ):\n",
    "            try:\n",
    "                dID_i = pair_i['id' ]\n",
    "                doc_i = pair_i['doc']\n",
    "                vec_i = pair_i['vec']\n",
    "\n",
    "\n",
    "                chunks_i = page_to_sentences( doc_i )\n",
    "                passgs_i = sentences_to_passages( chunks_i, lclEmbd, dID_i, segDiv = _PSG_DIST_DIV ) \n",
    "                \n",
    "                passgQ.extend( passgs_i )  # Safely append result\n",
    "    \n",
    "            except Exception as e:\n",
    "                print( f\"\\nBAD THING @ {os.getpid()}: {e}\\n\" )\n",
    "\n",
    "        m, s = divmod( now()-tBgn, 60.0  )\n",
    "        print( f\"\\nFINISHED processing passages @ {os.getpid()} after {int(m)}:{s:.2f}!\" )\n",
    "\n",
    "        with lock:\n",
    "            results.extend( list( passgQ ) )\n",
    "    \n",
    "        doc_queue.task_done()  # Signal task completion\n",
    "    \n",
    "\n",
    "def embed_passages_with_process_pool(documents: List[str], chunkSize: int = 10, num_workers: int = 4) -> List[Dict]:\n",
    "    # Initialize manager, queue, and shared resources\n",
    "    manager = multiprocessing.Manager()\n",
    "    doc_queue = multiprocessing.JoinableQueue()\n",
    "    results = manager.list()\n",
    "    # completed_chunks = manager.dict()\n",
    "    lock = manager.Lock()\n",
    "\n",
    "    # Start worker processes\n",
    "    processes = []\n",
    "    for _ in range(num_workers):\n",
    "        p = multiprocessing.Process(\n",
    "            target = passage_process_worker, \n",
    "            args   = (doc_queue, results, lock)\n",
    "        )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    # Enqueue documents with IDs\n",
    "\n",
    "    # Enqueue documents with IDs\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    Npgs = len( documents )\n",
    "    while bgn < Npgs:\n",
    "        end = min( bgn+chunkSize, Npgs )\n",
    "        doc_queue.put( deepcopy( documents[ bgn:end ] ) )\n",
    "        bgn = end\n",
    "    \n",
    "    # Block until all tasks are done\n",
    "    doc_queue.join()\n",
    "\n",
    "    # Stop workers\n",
    "    for _ in range( num_workers ):\n",
    "        doc_queue.put( None )  # Add a None for each worker to signal them to exit\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    return list(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "491b86a9-5e09-4954-ad06-0b58d109de1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import RAG_State\n",
    "GRGstate = RAG_State.load_state( environ[\"_RAG_STATE_PATH\"] )\n",
    "\n",
    "def bundle_pages( pageDcts, Ndoc ):\n",
    "    \"\"\" Prepare a subset of pages based on what has already been processed \"\"\"\n",
    "    global GRGstate\n",
    "    rtnBndl = deque()\n",
    "    count   = 0\n",
    "    for i, pair_i in enumerate ( pageDcts ):\n",
    "        dID_i = pair_i['id' ]\n",
    "        if dID_i not in GRGstate.passages['pages']:\n",
    "            rtnBndl.append( pair_i )\n",
    "            count += 1\n",
    "            if count >= Ndoc:\n",
    "                break\n",
    "    return list( rtnBndl )\n",
    "\n",
    "    \n",
    "def generate_passage_chunks( pageDcts, NpageBundle ):\n",
    "    \"\"\" Generate passages for as long as the user allows it \"\"\"\n",
    "    tBgn = now()\n",
    "    Ntot = 0\n",
    "    Npgs = 0\n",
    "    Nwrk = int( environ[\"_GRG_PASSAGE_N_WORKERS\"] ) \n",
    "    try:\n",
    "        while True:\n",
    "            ## Fetch Pages ##\n",
    "            pgBundle = bundle_pages( pageDcts, NpageBundle )\n",
    "            if len( pgBundle ) == 0:\n",
    "                raise KeyboardInterrupt( \"Ran out of input!\" )\n",
    "            ## Generate Passages ##\n",
    "            res = embed_passages_with_process_pool( \n",
    "                pgBundle, \n",
    "                chunkSize   = int(NpageBundle / Nwrk), \n",
    "                num_workers = Nwrk\n",
    "            )\n",
    "            if len( res ) > 0:\n",
    "                Npgs   += len( pgBundle )\n",
    "                Ntot   += len( res )\n",
    "                pklPath = f\"data/Passages_bgn-{pgBundle[0]['id']}.pkl\"\n",
    "                with open( pklPath, 'wb' ) as f:\n",
    "                    pickle.dump( res, f )\n",
    "                M, s = divmod( now()-tBgn, 60.0 )\n",
    "                h, m = divmod( M         , 60.0 )\n",
    "                print( f\"{int(h)}:{int(m)}:{s}, Saved {pklPath}!, {Npgs} --> {Ntot}\" )\n",
    "    except KeyboardInterrupt:\n",
    "        print( f\"\\nSTOPPED after {(now()-tBgn) / 3600.0} hours!\" )\n",
    "        print( f\"Segmented {Npgs} pages into {Ntot} passages!\" )\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a58b3ba-61e2-4175-824a-f8a8300f2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEGIN processing passages @ 9585!\n",
      "BEGIN processing passages @ 9588!\n",
      "BEGIN processing passages @ 9591!\n",
      "BEGIN processing passages @ 9594!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 9597!\n",
      "BEGIN processing passages @ 9603!\n",
      "\n",
      "BEGIN processing passages @ 9606!\n",
      "BEGIN processing passages @ 9600!\n",
      "BEGIN processing passages @ 9609!\n",
      "\n",
      "BEGIN processing passages @ 9612!\n",
      "\n",
      "BEGIN processing passages @ 9615!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 9618!\n",
      "\n",
      "\n",
      "BEGIN processing passages @ 9621!\n",
      "\n",
      "BEGIN processing passages @ 9624!\n",
      "\n",
      "BEGIN processing passages @ 9627!\n",
      "\n",
      "BEGIN processing passages @ 9630!\n",
      "\n",
      "FINISHED processing passages @ 9588 after 4:32.95!\n",
      "\n",
      "FINISHED processing passages @ 9609 after 8:20.92!\n",
      "\n",
      "FINISHED processing passages @ 9621 after 11:44.48!\n",
      "\n",
      "FINISHED processing passages @ 9630 after 14:26.56!\n",
      "\n",
      "FINISHED processing passages @ 9618 after 14:32.04!\n",
      "\n",
      "FINISHED processing passages @ 9603 after 15:5.35!\n",
      "\n",
      "FINISHED processing passages @ 9591 after 15:50.72!\n",
      "\n",
      "FINISHED processing passages @ 9600 after 16:21.23!\n",
      "\n",
      "FINISHED processing passages @ 9585 after 16:30.96!\n",
      "\n",
      "FINISHED processing passages @ 9594 after 16:32.20!\n",
      "\n",
      "FINISHED processing passages @ 9612 after 17:24.84!\n",
      "\n",
      "FINISHED processing passages @ 9597 after 18:47.33!\n",
      "\n",
      "FINISHED processing passages @ 9624 after 19:10.81!\n",
      "\n",
      "FINISHED processing passages @ 9627 after 19:16.51!\n",
      "\n",
      "FINISHED processing passages @ 9606 after 19:42.13!\n",
      "\n",
      "FINISHED processing passages @ 9615 after 19:55.92!\n",
      "\n",
      "#####\n",
      "Took 19:57.13 to process 160 pages! Segmented 6810 passages!\n"
     ]
    }
   ],
   "source": [
    "Nlrg = 160\n",
    "tBgn = now()\n",
    "res  = embed_passages_with_process_pool( vecPairs[:Nlrg], chunkSize= 10, num_workers = 16)\n",
    "m,s  = divmod( now()-tBgn, 60.0 )\n",
    "print( f\"\\n#####\\nTook {int(m)}:{s:.2f} to process {Nlrg} pages! Segmented {len(res)} passages!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95a734cb-62e9-41be-b552-33a89e3e1195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 6810\n",
      "{'id': '2decaed0-2849-4dd5-bdb5-90be3667111b',\n",
      " 'pageID': '50488d5f-16db-44fc-916c-92cda099bd04',\n",
      " 'txt': ' 11',\n",
      " 'vec': [-0.09438698,\n",
      "         0.04892953,\n",
      "         -0.078743376,\n",
      "         -0.032077055,\n",
      "         -0.01547839,\n",
      "         0.03400886,\n",
      "         0.05099287,\n",
      "         0.036503553,\n",
      "         -0.043149505,\n",
      "         -0.060115837,\n",
      "         -0.023042649,\n",
      "         -0.025040114,\n",
      "         -0.065213144,\n",
      "         -0.040461697,\n",
      "         -0.04994957,\n",
      "         -0.01922646,\n",
      "         -0.036449887,\n",
      "         -0.003360383,\n",
      "         -0.09124117,\n",
      "         -0.07770517,\n",
      "         0.017925603,\n",
      "         0.0067253304,\n",
      "         0.09170388,\n",
      "         0.027989198,\n",
      "         -0.0053046052,\n",
      "         -0.060477007,\n",
      "         -0.0057049952,\n",
      "         0.010500094,\n",
      "         -0.014267139,\n",
      "         -0.060952518,\n",
      "         0.060050737,\n",
      "         -0.00046423083,\n",
      "         0.07403473,\n",
      "         -0.046218734,\n",
      "         0.028839424,\n",
      "         -0.06655205,\n",
      "         -0.042915452,\n",
      "         -0.045748834,\n",
      "         0.0329275,\n",
      "         0.056266185,\n",
      "         -0.061798878,\n",
      "         -0.066599205,\n",
      "         -0.03895439,\n",
      "         -0.032675944,\n",
      "         0.025587019,\n",
      "         -0.061776012,\n",
      "         -0.05082949,\n",
      "         0.013641737,\n",
      "         0.030968588,\n",
      "         0.04226888,\n",
      "         -0.03119446,\n",
      "         0.044254497,\n",
      "         -0.05730737,\n",
      "         0.042859185,\n",
      "         0.036360953,\n",
      "         0.0037981619,\n",
      "         -0.13067271,\n",
      "         -0.033266783,\n",
      "         0.028940454,\n",
      "         -0.021697724,\n",
      "         0.04997291,\n",
      "         -0.028158559,\n",
      "         -0.059564598,\n",
      "         0.018136846,\n",
      "         -0.039781585,\n",
      "         -0.020738006,\n",
      "         -0.006196288,\n",
      "         -0.09408547,\n",
      "         -0.031189086,\n",
      "         0.038711056,\n",
      "         0.09966461,\n",
      "         0.051829364,\n",
      "         0.035696354,\n",
      "         -0.11295075,\n",
      "         0.045692984,\n",
      "         -0.070503704,\n",
      "         0.022504859,\n",
      "         -0.05078911,\n",
      "         -0.008434239,\n",
      "         -0.02972543,\n",
      "         -0.03125777,\n",
      "         -0.024347303,\n",
      "         -0.04848368,\n",
      "         0.006563099,\n",
      "         0.03292644,\n",
      "         -0.08775174,\n",
      "         0.056388382,\n",
      "         0.12644342,\n",
      "         -0.058808465,\n",
      "         0.07693157,\n",
      "         0.021841861,\n",
      "         0.028050886,\n",
      "         0.051734682,\n",
      "         0.0066826483,\n",
      "         -0.001436346,\n",
      "         0.0091194,\n",
      "         0.10873583,\n",
      "         -0.07144624,\n",
      "         -0.06075384,\n",
      "         0.24574256,\n",
      "         -0.0019843557,\n",
      "         0.036674738,\n",
      "         -0.026730616,\n",
      "         0.029113447,\n",
      "         -0.072226815,\n",
      "         0.08333754,\n",
      "         -0.0028406817,\n",
      "         0.0048275017,\n",
      "         -0.0117017,\n",
      "         -0.037885696,\n",
      "         -0.03550489,\n",
      "         0.0002132454,\n",
      "         -0.044839274,\n",
      "         -0.006027811,\n",
      "         0.028427202,\n",
      "         -0.021620272,\n",
      "         -0.023161031,\n",
      "         0.029755522,\n",
      "         0.01651054,\n",
      "         -0.02493487,\n",
      "         0.011281372,\n",
      "         -0.006806047,\n",
      "         0.009747371,\n",
      "         0.03125365,\n",
      "         -0.092064135,\n",
      "         -0.043592636,\n",
      "         -0.018072428,\n",
      "         -4.4231152e-33,\n",
      "         0.09402961,\n",
      "         -0.0458149,\n",
      "         -0.07811768,\n",
      "         0.07731379,\n",
      "         0.023809144,\n",
      "         0.040705573,\n",
      "         -0.059577703,\n",
      "         -0.013478647,\n",
      "         -0.045429952,\n",
      "         0.04260485,\n",
      "         -0.028186355,\n",
      "         0.0587932,\n",
      "         0.005717945,\n",
      "         -0.018320523,\n",
      "         0.047555234,\n",
      "         -0.02164074,\n",
      "         0.06883923,\n",
      "         0.05839881,\n",
      "         0.049073663,\n",
      "         -0.12120438,\n",
      "         -0.04738098,\n",
      "         -0.06194177,\n",
      "         -0.0065741977,\n",
      "         0.043955624,\n",
      "         -0.027106417,\n",
      "         0.011581295,\n",
      "         -0.07946261,\n",
      "         0.027273191,\n",
      "         0.05683023,\n",
      "         0.03735771,\n",
      "         -0.0130081065,\n",
      "         -0.04456198,\n",
      "         -0.0057798536,\n",
      "         0.047167785,\n",
      "         0.03298056,\n",
      "         -0.012974661,\n",
      "         0.121819444,\n",
      "         0.024284203,\n",
      "         0.042041786,\n",
      "         -0.13191952,\n",
      "         0.009225616,\n",
      "         0.03716925,\n",
      "         -0.022386711,\n",
      "         -0.009826946,\n",
      "         0.047689244,\n",
      "         0.039118595,\n",
      "         0.0780407,\n",
      "         -0.03590632,\n",
      "         0.060639583,\n",
      "         0.045820914,\n",
      "         -0.07008686,\n",
      "         -0.028214023,\n",
      "         -0.032985162,\n",
      "         -0.04351508,\n",
      "         -0.044936236,\n",
      "         -0.00087211636,\n",
      "         0.026972998,\n",
      "         -0.017373102,\n",
      "         0.03416765,\n",
      "         0.046781126,\n",
      "         0.121243484,\n",
      "         0.054750137,\n",
      "         -0.0006836636,\n",
      "         -0.001230737,\n",
      "         -0.030803742,\n",
      "         0.06474292,\n",
      "         0.055322446,\n",
      "         -0.00097542367,\n",
      "         0.08840172,\n",
      "         -0.0034624944,\n",
      "         -0.085551344,\n",
      "         0.026797622,\n",
      "         0.035008334,\n",
      "         0.029746817,\n",
      "         0.016943831,\n",
      "         -0.020046022,\n",
      "         0.07359015,\n",
      "         -0.051692553,\n",
      "         -0.0073362137,\n",
      "         0.12664288,\n",
      "         -0.009986726,\n",
      "         0.017702626,\n",
      "         -0.07088462,\n",
      "         0.04897783,\n",
      "         -0.009374926,\n",
      "         0.010865357,\n",
      "         -0.06869134,\n",
      "         -0.039747022,\n",
      "         0.011673271,\n",
      "         -0.0073038614,\n",
      "         0.04347287,\n",
      "         -0.09114517,\n",
      "         0.10489265,\n",
      "         -0.004024706,\n",
      "         0.014482685,\n",
      "         3.0370937e-33,\n",
      "         -0.036302507,\n",
      "         0.06069871,\n",
      "         -0.118739665,\n",
      "         0.015226239,\n",
      "         -0.046872936,\n",
      "         0.018428592,\n",
      "         0.06330217,\n",
      "         0.06993668,\n",
      "         -0.006509186,\n",
      "         -0.0002511339,\n",
      "         -0.030442994,\n",
      "         0.002970416,\n",
      "         0.040298164,\n",
      "         0.0058700074,\n",
      "         0.033876393,\n",
      "         0.016302356,\n",
      "         0.028624121,\n",
      "         0.021416496,\n",
      "         0.0034714679,\n",
      "         -0.023647543,\n",
      "         0.03383791,\n",
      "         0.021283085,\n",
      "         0.0015767791,\n",
      "         -0.061341237,\n",
      "         -0.049792215,\n",
      "         0.07837823,\n",
      "         0.04431642,\n",
      "         -0.00920988,\n",
      "         -0.0037311974,\n",
      "         0.01151443,\n",
      "         0.023495344,\n",
      "         -0.07577799,\n",
      "         0.004555036,\n",
      "         0.057744637,\n",
      "         -0.023640621,\n",
      "         -0.010209019,\n",
      "         0.042079076,\n",
      "         0.022910329,\n",
      "         -0.0654959,\n",
      "         0.010187906,\n",
      "         0.11874284,\n",
      "         -0.03153464,\n",
      "         -0.016148524,\n",
      "         0.111742064,\n",
      "         -0.019485299,\n",
      "         -0.030169316,\n",
      "         0.00210694,\n",
      "         0.018543534,\n",
      "         -0.034796353,\n",
      "         0.059834994,\n",
      "         -0.03435369,\n",
      "         -0.0014057407,\n",
      "         -0.03643227,\n",
      "         0.041467544,\n",
      "         -0.005253783,\n",
      "         0.008906187,\n",
      "         0.0061993115,\n",
      "         0.07827185,\n",
      "         0.060888812,\n",
      "         0.037948843,\n",
      "         0.073467515,\n",
      "         0.104288414,\n",
      "         -0.08930916,\n",
      "         0.037046444,\n",
      "         -0.029249955,\n",
      "         0.0692719,\n",
      "         -0.028864946,\n",
      "         0.009652959,\n",
      "         -0.03937594,\n",
      "         -0.014115907,\n",
      "         0.03033356,\n",
      "         0.02249724,\n",
      "         -0.07279786,\n",
      "         0.0032419576,\n",
      "         -0.06384635,\n",
      "         -0.122079454,\n",
      "         0.01656889,\n",
      "         0.008972487,\n",
      "         0.018252598,\n",
      "         0.014331656,\n",
      "         -0.07000115,\n",
      "         -0.019419873,\n",
      "         -0.017824098,\n",
      "         -0.047322378,\n",
      "         0.0028413613,\n",
      "         -0.06259787,\n",
      "         0.12733774,\n",
      "         0.07815883,\n",
      "         0.06387413,\n",
      "         0.06278425,\n",
      "         0.08009284,\n",
      "         0.0066356757,\n",
      "         0.06743912,\n",
      "         -0.031149987,\n",
      "         -0.01709898,\n",
      "         -1.2978951e-08,\n",
      "         -0.008861491,\n",
      "         0.034666996,\n",
      "         -0.034478586,\n",
      "         -0.050570183,\n",
      "         0.06495393,\n",
      "         0.038784593,\n",
      "         -0.05644793,\n",
      "         0.03521521,\n",
      "         0.042641398,\n",
      "         -0.008642123,\n",
      "         0.05893756,\n",
      "         -0.0046424414,\n",
      "         -0.022995884,\n",
      "         0.038441185,\n",
      "         -0.047074247,\n",
      "         -0.04820194,\n",
      "         0.030815436,\n",
      "         0.020001488,\n",
      "         -0.013496424,\n",
      "         -0.004951034,\n",
      "         -0.014047878,\n",
      "         0.026419485,\n",
      "         0.035964206,\n",
      "         -0.024139697,\n",
      "         -0.025273625,\n",
      "         -0.041354246,\n",
      "         0.0027546315,\n",
      "         0.03412145,\n",
      "         -0.0407458,\n",
      "         -0.023200063,\n",
      "         0.03917489,\n",
      "         0.09152177,\n",
      "         -0.06443007,\n",
      "         -0.1065287,\n",
      "         -0.028018655,\n",
      "         0.04123986,\n",
      "         0.030438537,\n",
      "         0.021477534,\n",
      "         -0.013565593,\n",
      "         0.020486718,\n",
      "         -0.009909966,\n",
      "         -0.051641807,\n",
      "         -0.03816899,\n",
      "         -0.05147939,\n",
      "         -0.04687135,\n",
      "         -0.048312128,\n",
      "         -0.1078664,\n",
      "         -0.021843215,\n",
      "         -0.027039638,\n",
      "         -0.0021366358,\n",
      "         -0.05491422,\n",
      "         0.027322847,\n",
      "         0.04065303,\n",
      "         0.005183037,\n",
      "         0.013633725,\n",
      "         0.0019562626,\n",
      "         0.017821437,\n",
      "         -0.0191542,\n",
      "         -0.045738,\n",
      "         0.08860285,\n",
      "         0.1269723,\n",
      "         0.037505265,\n",
      "         -0.0894628,\n",
      "         -0.015251594]}\n"
     ]
    }
   ],
   "source": [
    "print( type( res ), len( res ) )\n",
    "pprint( res[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f93215-60bf-4a76-b59f-e9b67799151d",
   "metadata": {},
   "source": [
    "## Link Passages to Parent Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5591b7-079f-43b2-abba-0c1f31c89951",
   "metadata": {},
   "source": [
    "## For each passage, Gather pages from N hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "205eb993-7aed-4f25-a61a-4e0a7d861e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in collection.get()['documents']:\n",
    "# # for result in collection.get()['ids']: # 6f83d661-6cdf-46bf-83a4-27f6c36d948f\n",
    "#     # print( result )\n",
    "#     print( dir( result ) )\n",
    "#     break\n",
    "#     # for res in result['ids']:\n",
    "#     #     all_ids.append( res )\n",
    "# # docIDs = list( all_ids )\n",
    "# # print( f\"There are {len(docIDs)} documents!\" )\n",
    "# # print( docIDs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40667b5d-ac67-4499-b9e6-c9d8f1b08f28",
   "metadata": {},
   "source": [
    "## Calc ranked passage similarity from those pages, Create up to M connections per passage\n",
    "### (This is a separate collection from page links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4408b94-4953-44fd-aa98-fb31e7bc44af",
   "metadata": {},
   "source": [
    "## Build Passage Graph @ ArangoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93294997-4586-4cef-918c-b9223d64c839",
   "metadata": {},
   "source": [
    "### Build Passage Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d881b33-1fbe-4c5b-adae-915db03ce34b",
   "metadata": {},
   "source": [
    "### Build Passage Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83748be8-aa1b-404b-8efd-b8a7ba1b0a57",
   "metadata": {},
   "source": [
    "# Create Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f70d6-f90d-416a-b6d1-7e427f17171b",
   "metadata": {},
   "source": [
    "## What decisions does the retriever have to make?\n",
    "* Following connections\n",
    "* Ranking pages and passages\n",
    "* Stitching passages in proper order.\n",
    "    - What is proper order?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dc867-db4b-4f3b-b654-1e735841ad30",
   "metadata": {},
   "source": [
    "## What is our token budget for the LLM summary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8de1f-c3d2-4446-8d72-ddc6c61c8289",
   "metadata": {},
   "source": [
    "## What would it look like to extend the budget with overlapping summaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6d349b-dd4d-4935-9e76-54256f8720de",
   "metadata": {},
   "source": [
    "# Create Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5804d7-6971-41b3-b303-78c3b21ab5da",
   "metadata": {},
   "source": [
    "## What would it look like for the summarizer to extend the token budget with Chain of Thought \"Reasoning\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e304759-e7fc-4735-ba76-36ed434e666c",
   "metadata": {},
   "source": [
    "# Depth 3: Build Statements (Assumptions and Claims) about Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905b2d8-6ba3-41c3-b1fd-a2f6b14c35a4",
   "metadata": {},
   "source": [
    "## Segment Keywords\n",
    "* Statistically important/rare phrases\n",
    "* Ask LLM to isolate {jargon, technical terms}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c77203-fff8-4db0-b1a3-5298ee1bf3ed",
   "metadata": {},
   "source": [
    "## Build Statements (Assumptions and Claims)\n",
    "* S-V-O sentence-by-sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd1625-3ff4-4a0e-bbb6-a3755650b554",
   "metadata": {},
   "source": [
    "# Depth 4: Support/Refute Statements (Assumptions and Claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193e0d7-3037-4eba-ab08-e2b574f4256d",
   "metadata": {},
   "source": [
    "## How to determine support?  How to determine contradiction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc56c5-df26-4305-997d-25950bcfaf88",
   "metadata": {},
   "source": [
    "## How to weigh support/contradiction based on our level of trust in existing documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe82a-21c1-410e-8757-11a0b5638820",
   "metadata": {},
   "source": [
    "# Advanced Knowledge Graph Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaf6d7-3549-40da-b28a-084eaf4b630b",
   "metadata": {},
   "source": [
    "## Statements: What is the difference between an Assumption and a Claim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6ca8f-a0b6-4abf-83a7-e81e5e22bbc2",
   "metadata": {},
   "source": [
    "## What assumptions do we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43bb88-9a67-4b51-8aac-a05aa43c2a43",
   "metadata": {},
   "source": [
    "## What claims are supported by assumptions we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f335b0-9f35-4cfd-b5ad-1c64252c7e91",
   "metadata": {},
   "source": [
    "## What questions are being asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b235e-2b90-4297-9ce0-c941a89c58c1",
   "metadata": {},
   "source": [
    "## What questions are being answered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0552f-3c89-4de4-9da9-a087489a16b2",
   "metadata": {},
   "source": [
    "## Does the document create new trustworthy connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3409c-8f9c-4a75-983f-5d4ab6c1a37b",
   "metadata": {},
   "source": [
    "## Can we find a similar subgraph in a different field of research?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286e3d5-056e-44af-bf9e-2d06941da2c9",
   "metadata": {},
   "source": [
    "## What questions CAN be asked based on the movement of passages and concepts through vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a9556-3072-4def-9292-31ff04f064b9",
   "metadata": {},
   "source": [
    "## Can we track trajectories in vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b0a23-d4c6-4aa6-8e44-8957200a58d5",
   "metadata": {},
   "source": [
    "## Can we PREDICT trajectories in vector space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e4882-5e86-4a22-905e-e4917c82e413",
   "metadata": {},
   "source": [
    "# KNNEST: Knowledge Graph Structure Notes\n",
    "* How to know sources are in different fields? ~ Cluster embeddings?\n",
    "* New heirarchical embedding per field?\n",
    "* Do embeddings need to be compressed by PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823f6b4-3876-4c77-995b-9de8c4419ecd",
   "metadata": {},
   "source": [
    "# ANTs: Search Agent Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acd01b-9797-4d5a-a9b4-f80985a4076e",
   "metadata": {},
   "source": [
    "## Can we TRAIN an agent to make advantageous traversals on the KG based on vector space deltas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d1b4b-db2e-43d7-bcf0-074a14e67167",
   "metadata": {},
   "source": [
    "## Swarm-Level Load Management\n",
    "* Agent Instantiation Condition(s)\n",
    "* Agent Deletion Condition(s)\n",
    "* Task Start Condition(s)\n",
    "* Task Stop Condition(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71fee0-1a30-4438-8049-75b49e2f0b75",
   "metadata": {},
   "source": [
    "## ANT Decision-Making Architecture\n",
    "* Resource alotment: {Time, Compute}\n",
    "* Critical: What edge to follow?\n",
    "* A strong line of reasoning can be a demonstration trajectory? LfD?\n",
    "* Inverse-RL to produce an evaluation function for trajectories thru the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22eaf-d4bf-45b4-bfae-3e54d4d12d97",
   "metadata": {},
   "source": [
    "# Connect to Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "606a31a3-ee34-4a8a-8fe7-0a5669e5a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# def start_arango_container():\n",
    "#     command = [ \"docker\", \"run\", \"-p\", \n",
    "#                 \"8529:8529\", \"-e\", \"ARANGO_ROOT_PASSWORD=\", \"arangodb/arangodb\"]  \n",
    "#     subprocess.Popen( command )\n",
    "\n",
    "# start_arango_container()\n",
    "# sleep( 15.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c1c39f5-d011-4487-8984-9869c3d7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate ArangoDB Database\n",
    "# import json\n",
    "\n",
    "# from adb_cloud_connector import get_temp_credentials\n",
    "# from arango import ArangoClient\n",
    "\n",
    "# con = get_temp_credentials()\n",
    "\n",
    "# db = ArangoClient(hosts=con[\"url\"]).db(\n",
    "#     con[\"dbName\"], con[\"username\"], con[\"password\"], verify=True\n",
    "# )\n",
    "\n",
    "# print(json.dumps(con, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ded7e008-5ddc-4bde-b666-cf6e9e8b9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the ArangoDB-LangChain Graph\n",
    "# from langchain_community.graphs import ArangoGraph\n",
    "\n",
    "# graph = ArangoGraph( db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94314a6a-941b-497b-941a-d95595615e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not db.has_graph( environ[\"_GRG_GRAPH_DB\"] ):\n",
    "#     db.create_graph(\n",
    "#         environ[\"_GRG_GRAPH_DB\"],\n",
    "#         edge_definitions=[\n",
    "#             {\n",
    "#                 \"from_vertex_collections\": [\"subjects\"],\n",
    "#                 \"edge_collection\": \"verbs\",\n",
    "#                 \"to_vertex_collections\": [\"subjects\"],\n",
    "#             },\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a70a3c7-ce94-41b8-9e1c-23a2fdf1761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import docker\n",
    "\n",
    "# client     = docker.from_env()\n",
    "# containers = client.containers.list()\n",
    "\n",
    "# for container in containers:\n",
    "#     print(container.name, container.short_id, container.status)\n",
    "\n",
    "# os.system( \"docker stop 8ed55910d8cc\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39eb5c6-6c53-4c30-a1e3-80ce01551f6c",
   "metadata": {},
   "source": [
    "# Wouldn't it be cool if ...\n",
    "* A manifold (high-order hypersurface) could be fit to the motion of human knowledge\n",
    "* We could trace paths on that manifold\n",
    "* Given a position and curvature (+jerk+snap+crackle+pop), we can extrapolate motion beyond the edge of the mapped manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799705-7295-4731-b6a8-ee7ba7dacabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
