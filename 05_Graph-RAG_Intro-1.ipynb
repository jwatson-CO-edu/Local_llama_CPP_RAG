{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90a2fda-d974-4e87-8038-f6c5b5bf47f3",
   "metadata": {},
   "source": [
    "# Resources\n",
    "* [AI-Scientist](https://github.com/SakanaAI/AI-Scientist )\n",
    "* [LangChain + Neo4j GRG Tutorial](https://python.langchain.com/docs/tutorials/graph/)\n",
    "* [Enhancing RAG-based application accuracy by constructing and leveraging knowledge graphs](https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09e7ba-2f31-4bf3-87b9-25cd6df751cf",
   "metadata": {},
   "source": [
    "# Preliminaries + Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f89225-fe56-4b1b-a1da-f8a2be6cdf84",
   "metadata": {},
   "source": [
    "These instructions are for Python 3.10\n",
    "### Install Notebook 04 Dependencies!\n",
    "* `sudo apt install docker.io`\n",
    "* `sudo chmod 666 /var/run/docker.sock`\n",
    "* `python3.10 -m pip install docker --user`\n",
    "### Install ArangoDB + Docker Container\n",
    "* `sudo docker pull arangodb`\n",
    "* `python3.10 -m pip install python-arango adb-cloud-connector --user`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c74f26-a5d8-4b32-9fa0-83d730041ede",
   "metadata": {},
   "source": [
    "# Inspiration\n",
    "* [Co-STORM @ Stanford](https://storm.genie.stanford.edu/)\n",
    "* [Ellicit](https://elicit.com/)\n",
    "* [Research Rabbit](https://www.researchrabbit.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec290487-1b04-4c74-9f95-a1e413cda7b7",
   "metadata": {},
   "source": [
    "# Init + Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87451cf5-6fc1-4a89-8644-84f31850ccdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## INIT ####################################################################################\n",
    "import os\n",
    "from os import path, makedirs, environ\n",
    "from utils import copy_pdfs\n",
    "\n",
    "\n",
    "\n",
    "########## ENVIRONMENT #############################################################################\n",
    "\n",
    "##### 04: Basic RAG #######################################################\n",
    "environ[\"_RAG_DOC_DBASE\"]  = \"lit_pdf\"\n",
    "environ[\"_RAG_DOC_EMBED\"]  = \"all-minilm\"\n",
    "\n",
    "##### 05: Graph-RAG (GRG) #################################################\n",
    "environ[\"_GRG_MODEL_NAME\"] = \"llama3.2-vision\"\n",
    "environ[\"_GRG_EMBED_NAME\"] = \"all-minilm\"\n",
    "\n",
    "environ[\"_GRG_GRAPH_DB\"] = \"grg_rel\"\n",
    "\n",
    "##### Flags ###############################################################\n",
    "_LINK_PAGES = True\n",
    "_DOC_EMBEDS = \"data/DocVectors.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a75e6de-09de-41c3-ad97-393d3e3d58ca",
   "metadata": {},
   "source": [
    "# Depth 1: Link PDF Pages by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de0c4d-5605-4d34-a271-c502f63be5f4",
   "metadata": {},
   "source": [
    "## Retrieve 04 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62a0cbf-525b-42d0-9031-e04e8d252f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Loading the vector store sometimes spews warnings\n",
    "import sys\n",
    "\n",
    "__import__('pysqlite3')\n",
    "sys.modules['sqlite3'] = sys.modules.pop( 'pysqlite3' )\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "persistent_client = chromadb.PersistentClient();\n",
    "collection        = persistent_client.get_or_create_collection( environ[\"_RAG_DOC_DBASE\"] );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b790070a-dbe5-4fed-b01f-8eb06dc69eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 213725 vectors!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vecPairs = None\n",
    "\n",
    "if _LINK_PAGES and os.path.isfile( _DOC_EMBEDS ):\n",
    "    with open( _DOC_EMBEDS, 'rb' ) as f:\n",
    "        vecPairs = pickle.load( f )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5557def-21c0-40a7-b871-ce4a6fafb090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to save 'all-minilm'.\n",
      "This will spew a lot of text on the first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 797b70c4edf8... 100% ▕████████████████▏  45 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 85011998c600... 100% ▕████████████████▏   16 B                         \n",
      "pulling 548455b72658... 100% ▕████████████████▏  407 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from utils import pull_ollama_model\n",
    "\n",
    "pull_ollama_model( environ[\"_RAG_DOC_EMBED\"] )\n",
    "\n",
    "local_embeddings = OllamaEmbeddings( model = environ[\"_RAG_DOC_EMBED\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c4b50-609c-45bc-a610-5fc5ecbeb226",
   "metadata": {},
   "source": [
    "## Recalculate Embeddings (NOT exposed by ChromaDB!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e42281-259a-4a8f-8432-138cc0fcfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you don’t have enough VRAM it will use the CPU. \n",
    "\n",
    "import time, os\n",
    "now = time.time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "\n",
    "    vecPairs = deque()\n",
    "    docBatch = 1\n",
    "\n",
    "    # Iterate over all documents and collect the IDs\n",
    "    all_ids = deque()\n",
    "    allData = collection.get()\n",
    "    totDocs = allData['documents']\n",
    "    totIDs  = allData['ids']\n",
    "    print( f\"Fetched {len(totDocs)} documents\" )\n",
    "    \n",
    "    tBgn = now()\n",
    "    N    = len( totDocs )\n",
    "    bgn  = 0\n",
    "    end  = 0\n",
    "    # for i, doc in enumerate( totDocs ):\n",
    "    while bgn < N:\n",
    "        end = min( bgn+docBatch, N )\n",
    "        try:\n",
    "            vec = local_embeddings.embed_documents( totDocs[ bgn:end ] )\n",
    "            for i in range( bgn, end ):\n",
    "                vecPairs.append( {'vec' : np.array( vec[i-bgn] ), 'doc': totDocs[i], 'id' : totIDs[i]} )\n",
    "                if ((i+1)%100) == 0:\n",
    "                    print('.',end='',flush=True)\n",
    "                if ((i+1)%10000) == 0:\n",
    "                    m,s = divmod( now()-tBgn, 60 )\n",
    "                    print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "            bgn = end\n",
    "        except Exception as e:\n",
    "            print(e,end=', ',flush=True)\n",
    "            bgn += 1\n",
    "    print( f\"\\nPage embedding recalc took {(now()-tBgn)/60.0:.2f} minutes!\" )\n",
    "\n",
    "    vecPairs = list( vecPairs )\n",
    "    print( f\"Got {len( vecPairs )} vectors!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1562139f-e122-4bcf-b428-89da709e41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    simMin = 1e9\n",
    "    simMax = 0.0\n",
    "    print( len(vecPairs) )\n",
    "    vectrs = [item['vec'] for item in vecPairs]\n",
    "    vec0   = vectrs[0]\n",
    "    \n",
    "    for vec_i in vectrs[1:]:\n",
    "        sim_i  = cosine_similarity( [vec0, vec_i,] )[0,1]\n",
    "        simMin = min( sim_i, simMin )\n",
    "        simMax = max( sim_i, simMax )\n",
    "    \n",
    "    diffSpan = simMax - simMin\n",
    "    \n",
    "    print( [simMin, simMax,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec7afa4e-5b50-46ad-8c81-988a654e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _LINK_PAGES and (not os.path.isfile( _DOC_EMBEDS )):\n",
    "    with open( _DOC_EMBEDS, 'wb' ) as f:\n",
    "        pickle.dump( vecPairs, f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003c93f9-9c84-461f-b299-790fce004f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIM_MAX         =  0.7329191963152777\n",
    "_SIM_MIN         = -0.24439346017677427\n",
    "_MAX_BRANCH      = 10\n",
    "_PAGE_CSN_THRESH = (_SIM_MAX-_SIM_MIN) * 0.95 + _SIM_MIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085508e0-626b-4f3a-b321-ce024b8e8e87",
   "metadata": {},
   "source": [
    "## Calculate Page Similarity && Build Page Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c952741-7fc6-46c8-9e40-76423919e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........~.."
     ]
    }
   ],
   "source": [
    "_PAGE_LINKS = \"data/PageLinks.pkl\"\n",
    "\n",
    "if _LINK_PAGES and (not os.path.isfile( _PAGE_LINKS )):\n",
    "    pageLinks = deque()\n",
    "    Ndocs     = len( vecPairs )\n",
    "    tBgn      = now()\n",
    "    for i in range( Ndocs-1 ):\n",
    "        if ((i+1)%10000) == 0:\n",
    "            m,s = divmod( now()-tBgn, 60 )\n",
    "            print(f\"\\n{i+1},{int(m)}:{s:.2f}\",end=' ',flush=True)\n",
    "        elif ((i+1)%1000==0):\n",
    "            print('>',end='',flush=True)\n",
    "        elif ((i+1)%100==0):\n",
    "            print('~',end='',flush=True)\n",
    "        elif ((i+1)%10==0):\n",
    "            print('.',end='',flush=True)\n",
    "        \n",
    "        pair_i = vecPairs[i]\n",
    "        Nconn  = 0\n",
    "        for j in range( i+1, Ndocs ):\n",
    "            pair_j = vecPairs[j]\n",
    "            sim_ij = cosine_similarity( [pair_i['vec'], pair_j['vec'],] )[0,1]\n",
    "            if sim_ij >= _PAGE_CSN_THRESH:\n",
    "                pageLinks.append( {\n",
    "                    'type'   : \"Page_Cosine_Similarity\",\n",
    "                    'idTail' : pair_i['id'],\n",
    "                    'idHead' : pair_j['id'],\n",
    "                    'dir'    : False,\n",
    "                    'coSim'  : sim_ij,\n",
    "                } )\n",
    "                Nconn += 1\n",
    "                if Nconn >= _MAX_BRANCH:\n",
    "                    break\n",
    "    pageLinks = list( pageLinks )\n",
    "    print( f\"\\nBuilt page graph in {(now()-tBgn)/60.0/60.0:.2f} hours!\" )\n",
    "    print()\n",
    "\n",
    "    with open( _PAGE_LINKS, 'wb' ) as f:\n",
    "        pickle.dump( pageLinks, f )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5117a2e-6a1d-4f3c-9258-6a5da33c31b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME, START HERE: CONNECT TO GRAPH DB \n",
    "    # * STORE DOCUMENTS WITH EMBEDDINGS\n",
    "    # * STORE EDGES WITH SIMILARITY\n",
    "\n",
    "# FIXME: SAVE THE DB AND BACK UP THE FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d0337e-e003-45c0-9a5d-ca042a5191cc",
   "metadata": {},
   "source": [
    "# Depth 2: Link Passages by Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f569ef3-fecf-4c82-be8c-181546e32fe7",
   "metadata": {},
   "source": [
    "## Segment Pages Into Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5d91a-5faf-4222-8195-7a1b36dc1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_to_sentences( pageText ):\n",
    "    \"\"\" Parse the page into individual sentences \"\"\"\n",
    "    rtnParts = deque()\n",
    "    pageText = str( pageText ) + '.' # Terminator hack\n",
    "    sepChars = ['.','?','!']\n",
    "    sentence = \"\"\n",
    "\n",
    "    # FIXME, START HERE: A PERIOD MUST BE FOLLOWED BY AT LEAST A SPACE OTHERWISE SENTENCES END AT DECIMALS\n",
    "\n",
    "    def push_chunk():\n",
    "        nonlocal rtnParts, sentence\n",
    "        rtnParts.append( sentence )\n",
    "        sentence = \"\"\n",
    "    \n",
    "    for c in pageText:\n",
    "        sentence += c # Include punctuation in the sentence\n",
    "        if c in sepChars:\n",
    "            push_chunk()\n",
    "\n",
    "    chunks = list( rtnParts )\n",
    "\n",
    "    # FIXME: REMOVE LEADING/TRAILING SPACE\n",
    "    \n",
    "    return chunks \n",
    "\n",
    "\n",
    "# FIXME: REMEMBER TO LINK PASSAGES TO THEIR PARENT PAGES!!!!!\n",
    "\n",
    "\n",
    "def sentences_to_passages( chunkList, embedder, segDiv = 10.0 ):\n",
    "    \"\"\" Segment a list of sentences into a passage \"\"\"\n",
    "    vectors = embedder.embed_documents( chunkList )\n",
    "    senVecs = zip( vectors, chunkList )\n",
    "    rtnSeg  = deque()\n",
    "\n",
    "    def vec_diff( v1, v2 ):\n",
    "        \"\"\" Distance between 2 pnts \"\"\"\n",
    "        return np.linalg.norm( np.subtract( v1, v2 ) )\n",
    "\n",
    "    def get_total_width():\n",
    "        nonlocal vectors\n",
    "        N    = len( vectors )\n",
    "        dMax = -1.0\n",
    "        for i in range( N-1 ):\n",
    "            for j in range( i+1, N ):\n",
    "                dMax = max( dMax, vec_diff( vectors[i], vectors[j] ) )\n",
    "        return dMax\n",
    "\n",
    "    segRad = get_total_width() / segDiv\n",
    "    \n",
    "    vecs_j = list()\n",
    "    mean_j = None\n",
    "    txtP_j = \"\"\n",
    "\n",
    "    for i, (vec_i, sen_i) in enumerate( senVecs ):\n",
    "\n",
    "        if i == 0:\n",
    "            mean_j = vec_i\n",
    "\n",
    "        if vec_diff( vec_i, mean_j ) > segRad:\n",
    "            rtnSeg.append({\n",
    "                'vec' : embedder.embed_documents( [txtP_j,] )[0], # Embedding might be different than the average?\n",
    "                'txt' : txtP_j,\n",
    "            })\n",
    "            vecs_j = [vec_i,]\n",
    "            mean_j = vec_i\n",
    "            txtP_j = sen_i\n",
    "        else:\n",
    "            vecs_j.append( vec_i )\n",
    "            mean_j = np.mean( vecs_j, axis = 0 )\n",
    "            txtP_j += ' ' + sen_i # Reinsert leading space\n",
    "\n",
    "    return list( rtnSeg )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205eb993-7aed-4f25-a61a-4e0a7d861e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in collection.get()['documents']:\n",
    "# # for result in collection.get()['ids']: # 6f83d661-6cdf-46bf-83a4-27f6c36d948f\n",
    "#     # print( result )\n",
    "#     print( dir( result ) )\n",
    "#     break\n",
    "#     # for res in result['ids']:\n",
    "#     #     all_ids.append( res )\n",
    "# # docIDs = list( all_ids )\n",
    "# # print( f\"There are {len(docIDs)} documents!\" )\n",
    "# # print( docIDs[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e304759-e7fc-4735-ba76-36ed434e666c",
   "metadata": {},
   "source": [
    "# Depth 3: Build Statements (Assumptions and Claims) about Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905b2d8-6ba3-41c3-b1fd-a2f6b14c35a4",
   "metadata": {},
   "source": [
    "## Segment Keywords\n",
    "* Statistically important/rare phrases\n",
    "* Ask LLM to isolate {jargon, technical terms}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c77203-fff8-4db0-b1a3-5298ee1bf3ed",
   "metadata": {},
   "source": [
    "## Build Statements (Assumptions and Claims)\n",
    "* S-V-O sentence-by-sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd1625-3ff4-4a0e-bbb6-a3755650b554",
   "metadata": {},
   "source": [
    "# Depth 4: Support/Refute Statements (Assumptions and Claims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe82a-21c1-410e-8757-11a0b5638820",
   "metadata": {},
   "source": [
    "# Advanced Knowledge Graph Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaf6d7-3549-40da-b28a-084eaf4b630b",
   "metadata": {},
   "source": [
    "## Statements: What is the difference between an Assumption and a Claim?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6ca8f-a0b6-4abf-83a7-e81e5e22bbc2",
   "metadata": {},
   "source": [
    "## What assumptions do we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43bb88-9a67-4b51-8aac-a05aa43c2a43",
   "metadata": {},
   "source": [
    "## What claims are supported by assumptions we trust?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f335b0-9f35-4cfd-b5ad-1c64252c7e91",
   "metadata": {},
   "source": [
    "## What questions are being asked?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b235e-2b90-4297-9ce0-c941a89c58c1",
   "metadata": {},
   "source": [
    "## What questions are being answered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0552f-3c89-4de4-9da9-a087489a16b2",
   "metadata": {},
   "source": [
    "## Does the document create new trustworthy connections?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3409c-8f9c-4a75-983f-5d4ab6c1a37b",
   "metadata": {},
   "source": [
    "## Can we find a similar subgraph in a different field of research?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e4882-5e86-4a22-905e-e4917c82e413",
   "metadata": {},
   "source": [
    "# KNNEST: Knowledge Graph Structure Notes\n",
    "* How to know sources are in different fields?\n",
    "* New heirarchical embedding per field?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d823f6b4-3876-4c77-995b-9de8c4419ecd",
   "metadata": {},
   "source": [
    "# ANTs: Search Agent Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129d1b4b-db2e-43d7-bcf0-074a14e67167",
   "metadata": {},
   "source": [
    "## Swarm-Level Load Management\n",
    "* Agent Instantiation Condition(s)\n",
    "* Agent Deletion Condition(s)\n",
    "* Task Start Condition(s)\n",
    "* Task Stop Condition(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71fee0-1a30-4438-8049-75b49e2f0b75",
   "metadata": {},
   "source": [
    "## ANT Decision-Making Architecture\n",
    "* Resource alotment: {Time, Compute}\n",
    "* Critical: What edge to follow?\n",
    "* A strong line of reasoning can be a demonstration trajectory? LfD?\n",
    "* Inverse-RL to produce an evaluation function for trajectories thru the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a22eaf-d4bf-45b4-bfae-3e54d4d12d97",
   "metadata": {},
   "source": [
    "# Connect to Graph Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a31a3-ee34-4a8a-8fe7-0a5669e5a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from time import sleep\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# def start_arango_container():\n",
    "#     command = [ \"docker\", \"run\", \"-p\", \n",
    "#                 \"8529:8529\", \"-e\", \"ARANGO_ROOT_PASSWORD=\", \"arangodb/arangodb\"]  \n",
    "#     subprocess.Popen( command )\n",
    "\n",
    "# start_arango_container()\n",
    "# sleep( 15.0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c39f5-d011-4487-8984-9869c3d7981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate ArangoDB Database\n",
    "# import json\n",
    "\n",
    "# from adb_cloud_connector import get_temp_credentials\n",
    "# from arango import ArangoClient\n",
    "\n",
    "# con = get_temp_credentials()\n",
    "\n",
    "# db = ArangoClient(hosts=con[\"url\"]).db(\n",
    "#     con[\"dbName\"], con[\"username\"], con[\"password\"], verify=True\n",
    "# )\n",
    "\n",
    "# print(json.dumps(con, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7e008-5ddc-4bde-b666-cf6e9e8b9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the ArangoDB-LangChain Graph\n",
    "# from langchain_community.graphs import ArangoGraph\n",
    "\n",
    "# graph = ArangoGraph( db )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94314a6a-941b-497b-941a-d95595615e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not db.has_graph( environ[\"_GRG_GRAPH_DB\"] ):\n",
    "#     db.create_graph(\n",
    "#         environ[\"_GRG_GRAPH_DB\"],\n",
    "#         edge_definitions=[\n",
    "#             {\n",
    "#                 \"from_vertex_collections\": [\"subjects\"],\n",
    "#                 \"edge_collection\": \"verbs\",\n",
    "#                 \"to_vertex_collections\": [\"subjects\"],\n",
    "#             },\n",
    "#         ],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70a3c7-ce94-41b8-9e1c-23a2fdf1761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import docker\n",
    "\n",
    "# client     = docker.from_env()\n",
    "# containers = client.containers.list()\n",
    "\n",
    "# for container in containers:\n",
    "#     print(container.name, container.short_id, container.status)\n",
    "\n",
    "# os.system( \"docker stop 8ed55910d8cc\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b231d76-142d-4fb0-a81b-34f67170d0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799705-7295-4731-b6a8-ee7ba7dacabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
